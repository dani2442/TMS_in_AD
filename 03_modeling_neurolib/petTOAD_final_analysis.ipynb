{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"   Analyse simulations   -- Version 1.0\n",
    "Last edit:  2023/08/29\n",
    "Authors:    Leone, Riccardo (RL)\n",
    "Notes:      - Analyse all the models\n",
    "            - Release notes:\n",
    "                * Initial commit\n",
    "To do:      - \n",
    "Comments:   \n",
    "\n",
    "Sources: \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "from petTOAD_setup import *\n",
    "from statannotations.Annotator import Annotator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_DIR = RES_DIR / \"final_simulations\"\n",
    "A_DIR = SIM_DIR / \"a-weight_ws_-0.1-0.1_bs_-0.05-0.05\"\n",
    "A_RAND_DIR = SIM_DIR / \"a-weight_ws_-0.1-0.1_bs_-0.05-0.05_random\"\n",
    "G_DIR = SIM_DIR / \"G-weight_ws_-1.0-1.0_bs_-0.5-0.5\"\n",
    "G_RAND_DIR = SIM_DIR / \"G-weight_ws_-1.0-1.0_bs_-0.5-0.5_random\"\n",
    "SC_DIR = SIM_DIR / \"sc_disconn\"\n",
    "HET_DIR = SIM_DIR / \"heterogeneous_ws_-0.1-0.1_bs_-0.05-0.05\"\n",
    "HET_RAND_DIR = SIM_DIR / \"heterogeneous_ws_-0.1-0.1_bs_-0.05-0.05_random\"\n",
    "FIG_DIR = SIM_DIR / \"Figures\"\n",
    "if not Path.exists(FIG_DIR):\n",
    "    Path.mkdir(FIG_DIR)\n",
    "MCI_DIR = RES_DIR / \"model_simulations\" / \"MCI_noWMH\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_star(tbl):\n",
    "    \"\"\"\n",
    "    This function is just to plot an asterisk at the best heatmap value for better recognition.\n",
    "    \"\"\"\n",
    "    star = tbl.where(tbl == tbl.values.max())\n",
    "    star = star.replace({np.nan: \"\"})\n",
    "    star = star.replace({tbl.values.max(): \"*\"})\n",
    "    return star\n",
    "\n",
    "\n",
    "def find_max_b_w_tbl(tbl):\n",
    "    \"\"\"\n",
    "    This function finds the w and b that are associated with the star (best value) in the heatmap of group comparisons\n",
    "    \"\"\"\n",
    "    max_value = tbl.values.max()\n",
    "    mask = tbl == max_value\n",
    "    column_names = tbl.columns[mask.any(axis=0)]\n",
    "    column_floats = [float(col) for col in column_names]\n",
    "    row_names = tbl.index[mask.any(axis=1)]\n",
    "    row_floats = [float(row) for row in row_names]\n",
    "    return row_floats[0], column_floats[0]\n",
    "\n",
    "\n",
    "def get_best_dfs(big_df, fc_name, phfcd_name, comp_score_name, wmh_dict):\n",
    "    big_df[\"comp_score\"] = big_df[\"fc_pearson\"] + 1 * big_df[\"phfcd_ks\"]\n",
    "    # Get the best model fits for fc, phfcd for each subject and create one single df (skip fcd for now)\n",
    "    res_df_best = pd.DataFrame(\n",
    "        {\n",
    "            fc_name: big_df.groupby([\"PTID\"])[fc_name].max(),\n",
    "            phfcd_name: big_df.groupby([\"PTID\"])[phfcd_name].max(),\n",
    "            comp_score_name: big_df.groupby([\"PTID\"])[comp_score_name].max(),\n",
    "        }\n",
    "    ).reset_index()\n",
    "\n",
    "    res_df_best[\"wmh_load\"] = [wmh_dict[subj] for subj in res_df_best[\"PTID\"]]\n",
    "    # This df is to check what are the best w, b in case of the best fc, best phfcd and best glob score\n",
    "    # First we group by each patient and get the max fc pearson for each\n",
    "    # Then we merge this df (with the name of pt and max fc) with the df containing also b and w\n",
    "    # so we have: max(FC), b, w for each patient etc.\n",
    "    max_fc_vals = big_df.groupby([\"PTID\"])[fc_name].max().reset_index()\n",
    "    merged_df_max_fc = pd.merge(big_df, max_fc_vals, on=[\"PTID\", fc_name])\n",
    "    # We do the same for 1-phFCD\n",
    "    max_phfcd_vals = big_df.groupby([\"PTID\"])[phfcd_name].max().reset_index()\n",
    "    merged_df_max_phfcd = pd.merge(big_df, max_phfcd_vals, on=[\"PTID\", phfcd_name])\n",
    "    # We do the same for GFS\n",
    "    max_comp_score_vals = big_df.groupby([\"PTID\"])[comp_score_name].max().reset_index()\n",
    "    merged_df_max_comp_score = pd.merge(big_df, max_comp_score_vals, on=[\"PTID\", comp_score_name])\n",
    "\n",
    "\n",
    "    return merged_df_max_fc, merged_df_max_phfcd, merged_df_max_comp_score\n",
    "\n",
    "\n",
    "def plot_parm_best_wmh_relationship(parm, df_max_comp_score):\n",
    "    \"\"\"\n",
    "    This function plots the relationship between the best achieved FC and (1-phFCD) and WMH (normalized) in the parameter space that was explored\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(7, 7))\n",
    "    # Values that give the best FC\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 1],\n",
    "        data=df_max_comp_score[df_max_comp_score[\"PTID\"].isin(CN_WMH)],\n",
    "        x=\"wmh_load\",\n",
    "        y=parm,\n",
    "        label=\"CU WMH\",\n",
    "    )\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 1],\n",
    "        data=df_max_comp_score[df_max_comp_score[\"PTID\"].isin(MCI_WMH)],\n",
    "        x=\"wmh_load\",\n",
    "        y=parm,\n",
    "        label=\"MCI WMH\",\n",
    "    )\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "def create_pivot_tables(df_res):\n",
    "    # Convert the result df into a pivot table so to plot heatmap\n",
    "    table_fc = pd.pivot_table(\n",
    "        df_res, values=\"fc_pearson\", index=\"b\", columns=\"w\"\n",
    "    ).astype(float)\n",
    "    # Skip fcd for now\n",
    "    # table_fcd = pd.pivot_table(res_df, values='fcd_ks', index='b', columns='w').astype(float)\n",
    "    table_phfcd = pd.pivot_table(\n",
    "        df_res, values=\"phfcd_ks\", index=\"b\", columns=\"w\"\n",
    "    ).astype(float)\n",
    "    # Create a composite score by summing up the single model fits so to choose when FC and phFCD have different \"best\" b-w combinations\n",
    "    table_comp_score = table_fc + 1 * table_phfcd\n",
    "    return table_fc, table_phfcd, table_comp_score\n",
    "\n",
    "\n",
    "def plot_save_heatmaps(table_fc, table_phfcd, table_comp_score, random):\n",
    "    \"\"\"\n",
    "    This function starts from the results dataframe, converts it into a pivot-table, which is what seaborn need to plot the heatmap.\n",
    "    It does so both for fc and for phfcd. Also plots and saves the results.\n",
    "    \"\"\"\n",
    "    # Create Heatmaps\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    sns.heatmap(\n",
    "        ax=axs[0],\n",
    "        data=table_fc,\n",
    "        annot=annotate_star(table_fc),\n",
    "        fmt=\"\",\n",
    "        annot_kws={\"size\": 10},\n",
    "    )\n",
    "    axs[0].set_title(f\"FC\", fontsize=9)\n",
    "\n",
    "    sns.heatmap(\n",
    "        ax=axs[1],\n",
    "        data=table_phfcd,\n",
    "        annot=annotate_star(table_phfcd),\n",
    "        fmt=\"\",\n",
    "        annot_kws={\"size\": 10},\n",
    "    )\n",
    "    axs[1].set_title(f\"1 - phFCD\", fontsize=9)\n",
    "    sns.heatmap(\n",
    "        ax=axs[2],\n",
    "        data=table_comp_score,\n",
    "        annot=annotate_star(table_comp_score),\n",
    "        fmt=\"\",\n",
    "        annot_kws={\"size\": 10},\n",
    "    )\n",
    "    axs[2].set_title(f\"GFS\", fontsize=9)\n",
    "    if not random:\n",
    "        savename = f\"weights_and_bias_heatmap.png\"\n",
    "        fig.suptitle(\"Random = False\")\n",
    "    else:\n",
    "        savename = f\"weights_and_bias_heatmap_random.png\"\n",
    "        fig.suptitle(\"Random = True\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(FIG_DIR / savename)\n",
    "\n",
    "\n",
    "def perform_group_analyses(df_petTOAD, big_df, random):\n",
    "    \"\"\"\n",
    "    This function allow us to select the grouping classification (Fazekas 1, Fazekas 2 or all) as the 'group'variable and then it selects\n",
    "    only CN and MCI with WMH based on the chosen classification (since we are not interested for this type of model in patients without WMH).\n",
    "    Then, calls the save_plot_results function, which plots and saves a summary heatmap of the mean FC and 1-phFCD across all patients that fall\n",
    "    into a category (CN or MCI) according to the chosen grouping.\n",
    "\n",
    "    Args:\n",
    "        df_petTOAD:\n",
    "        big_df:\n",
    "        group:\n",
    "        random:\n",
    "\n",
    "    Returns:\n",
    "        The b and ws associated with\n",
    "    \"\"\"\n",
    "    CN_WMH = df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"CN_WMH\"][\"PTID\"]\n",
    "    MCI_WMH = df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"][\"PTID\"]\n",
    "\n",
    "    CN_wmh_df = big_df[big_df[\"PTID\"].isin(CN_WMH)]\n",
    "    CN_wmh_grouped = CN_wmh_df.drop(columns=[\"PTID\"]).groupby([\"b\", \"w\"]).mean()\n",
    "    tbl_fc_CN_wmh, tbl_phfcd_CN_wmh, tbl_comp_score_CN_wmh = create_pivot_tables(\n",
    "        CN_wmh_grouped\n",
    "    )\n",
    "    plot_save_heatmaps(\n",
    "        tbl_fc_CN_wmh,\n",
    "        tbl_phfcd_CN_wmh,\n",
    "        tbl_comp_score_CN_wmh,\n",
    "        random,\n",
    "    )\n",
    "\n",
    "    mci_wmh_df = big_df[big_df[\"PTID\"].isin(MCI_WMH)]\n",
    "    mci_wmh_grouped = mci_wmh_df.drop(columns=[\"PTID\"]).groupby([\"b\", \"w\"]).mean()\n",
    "    tbl_fc_mci_wmh, tbl_phfcd_mci_wmh, tbl_comp_score_mci_wmh = create_pivot_tables(\n",
    "        mci_wmh_grouped\n",
    "    )\n",
    "    plot_save_heatmaps(\n",
    "        tbl_fc_mci_wmh,\n",
    "        tbl_phfcd_mci_wmh,\n",
    "        tbl_comp_score_mci_wmh,\n",
    "        random,\n",
    "    )\n",
    "\n",
    "    b_CN_wmh, w_CN_wmh = find_max_b_w_tbl(tbl_comp_score_CN_wmh)\n",
    "    b_mci_wmh, w_mci_wmh = find_max_b_w_tbl(tbl_comp_score_mci_wmh)\n",
    "\n",
    "    print(\"The best parameters found for these heatmaps are:\")\n",
    "    print(f\"b = {b_CN_wmh}, w = {w_CN_wmh}, for CN with WMH\")\n",
    "    print(f\"b = {b_mci_wmh}, w = {w_mci_wmh}, for MCI with WMH\")\n",
    "\n",
    "    return b_CN_wmh, w_CN_wmh, b_mci_wmh, w_mci_wmh\n",
    "\n",
    "\n",
    "def select_b_w_combo(df_boxplot_long, w1, b1, w2, b2, groupname1, groupname2):\n",
    "    compare_df1 = df_boxplot_long[\n",
    "        (df_boxplot_long[\"w\"] == w1) & (df_boxplot_long[\"b\"] == b1)\n",
    "    ]\n",
    "    compare_df1 = compare_df1[compare_df1[\"group\"] == groupname1]\n",
    "    compare_df2 = df_boxplot_long[\n",
    "        (df_boxplot_long[\"w\"] == w2) & (df_boxplot_long[\"b\"] == b2)\n",
    "    ]\n",
    "    compare_df2 = compare_df2[compare_df2[\"group\"] == groupname2]\n",
    "    joined_compare_df = pd.concat([compare_df1, compare_df2])\n",
    "    return compare_df1, compare_df2, joined_compare_df\n",
    "\n",
    "\n",
    "def create_df_boxplot(df_big, best_fit_b, best_fit_w, diagnosis_group_wmh, model_type):\n",
    "    \"\"\"\n",
    "    This function starts from the big database obtained from the model simulation (e.g., a-weighted homogenous, G-weighted homgoeneous, heterogeneous, etc.) and creates\n",
    "    a smaller dataset with only the observable variables obtained with the best b and w combination as before. It is useful to then create a long df for plotting.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df[\"PTID\"] = df_big[\n",
    "        (df_big[\"b\"] == best_fit_b)\n",
    "        & (df_big[\"w\"] == best_fit_w)\n",
    "        & (df_big[\"Group_Fazekas_2.0\"] == diagnosis_group_wmh)\n",
    "    ][\"PTID\"]\n",
    "    df[\"fc_pearson\"] = df_big[\n",
    "        (df_big[\"b\"] == best_fit_b)\n",
    "        & (df_big[\"w\"] == best_fit_w)\n",
    "        & (df_big[\"Group_Fazekas_2.0\"] == diagnosis_group_wmh)\n",
    "    ][\"fc_pearson\"]\n",
    "    df[\"phfcd_ks\"] = df_big[\n",
    "        (df_big[\"b\"] == best_fit_b)\n",
    "        & (df_big[\"w\"] == best_fit_w)\n",
    "        & (df_big[\"Group_Fazekas_2.0\"] == diagnosis_group_wmh)\n",
    "    ][\"phfcd_ks\"]\n",
    "    df[\"comp_score\"] = df[\"fc_pearson\"] + 1 * df[\"phfcd_ks\"]\n",
    "    df[\"Model_type\"] = [f\"{model_type}\" for _ in range(len(df))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_obs(df, obs, ax):\n",
    "    \"\"\"\n",
    "    This function plots the annotations for statistical comparisons between groups\n",
    "    \"\"\"\n",
    "    order = [\"Base\", \"Homo.\", \"Hetero.\", \"G-weight\", \"Disconn.\"]\n",
    "    # Choose the combinations that you want to test..\n",
    "    pairs = [\n",
    "        (\"Base\", \"Homo.\"),\n",
    "        (\"Base\", \"Hetero.\"),\n",
    "        (\"Base\", \"G-weight\"),\n",
    "        (\"Base\", \"Disconn.\"),\n",
    "        (\"Homo.\", \"Hetero.\"),\n",
    "        (\"Homo.\", \"G-weight\"),\n",
    "        (\"Hetero.\", \"G-weight\"),\n",
    "    ]\n",
    "    f = sns.boxplot(data=df, x=\"Model_type\", y=obs, ax=ax)\n",
    "    annotator = Annotator(f, pairs, data=df, x=\"Model_type\", y=obs, order=order)\n",
    "    # Choose the type of statistical test to perform\n",
    "    annotator.configure(test=\"Wilcoxon\", text_format=\"star\", loc=\"inside\", verbose=1)\n",
    "    annotator.apply_and_annotate()\n",
    "    plt.xticks([0, 1, 2, 3, 4], order)\n",
    "    plt.xlabel(\"Model type\")\n",
    "\n",
    "\n",
    "def plot_random_comparison(df, obs, axs):\n",
    "    \"\"\"\n",
    "    This function is for plotting the comparison between the model results achieved in CN and MCI with the same model\n",
    "    \"\"\"\n",
    "    pairs = [\n",
    "        ((\"Homo.\", \"Random\"), (\"Homo.\", \"Not Random\")),\n",
    "        ((\"Hetero.\", \"Random\"), (\"Hetero.\", \"Not Random\")),\n",
    "        ((\"G-weight\", \"Random\"), (\"G-weight\", \"Not Random\")),\n",
    "    ]\n",
    "\n",
    "    # This is a dictionary of arguments that are passed into the function inside map_dataframe\n",
    "    kwargs = {\n",
    "        \"plot_params\": {  # this takes what normally goes into sns.barplot etc.\n",
    "            \"x\": \"Model_type\",\n",
    "            \"y\": f\"{obs}\",\n",
    "            \"hue\": \"Random\",\n",
    "            \"hue_order\": [\"Random\", \"Not Random\"],\n",
    "        },\n",
    "        \"annotation_func\": \"apply_test\",\n",
    "        \"configuration\": {\n",
    "            \"test\": \"Wilcoxon\",\n",
    "        },  # this takes what normally goes into ant.configure\n",
    "        \"plot\": \"boxplot\",\n",
    "        # If you want to add the figure into a subplot...\n",
    "        \"ax\": axs,\n",
    "    }\n",
    "\n",
    "    df[\"Model_type\"] = df[\"Model_type\"].str.replace(\"Homo. rand.\", \"Homo.\")\n",
    "    df[\"Model_type\"] = df[\"Model_type\"].str.replace(\"Hetero. rand.\", \"Hetero.\")\n",
    "    df[\"Model_type\"] = df[\"Model_type\"].str.replace(\"G rand.\", \"G-weight\")\n",
    "\n",
    "    ant = Annotator(None, pairs)\n",
    "    # We create a FacetGrid and pass the dataframe that we want to use to later apply our functions (plotting the comparisons between groups)\n",
    "    g1 = sns.FacetGrid(df, aspect=1.5, height=4)\n",
    "    # map_dataframe accepts a function, which it then applies to the dataframe that is previously passed in the FacetGrid. It also accepts kwargs which\n",
    "    # are passed inside the function\n",
    "    g1.map_dataframe(ant.plot_and_annotate_facets, **kwargs)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_CN_mci_comparison(obs, df_compare_groups): # (obs, axs) if you want subplots\n",
    "    \"\"\"\n",
    "    This function is for plotting the comparison between the model results achieved in CN and MCI with the same model\n",
    "    \"\"\"\n",
    "    df_compare_groups[\"group\"] = df_compare_groups[\"group\"].str.replace(\"CN\", \"CU\") #Change to CU so that the label is already ok\n",
    "    pairs = [\n",
    "        ((\"Base\", \"CU WMH\"), (\"Base\", \"MCI WMH\")),\n",
    "        ((\"Homo.\", \"CU WMH\"), (\"Homo.\", \"MCI WMH\")),\n",
    "        ((\"Hetero.\", \"CU WMH\"), (\"Hetero.\", \"MCI WMH\")),\n",
    "        ((\"G-weight\", \"CU WMH\"), (\"G-weight\", \"MCI WMH\")),\n",
    "        ((\"Disconn.\", \"CU WMH\"), (\"Disconn.\", \"MCI WMH\")),\n",
    "    ]\n",
    "\n",
    "    # This is a dictionary of arguments that are passed into the function inside map_dataframe\n",
    "    kwargs = {\n",
    "        \"plot_params\": {  # this takes what normally goes into sns.barplot etc.\n",
    "            \"x\": \"Model_type\",\n",
    "            \"y\": f\"{obs}\",\n",
    "            \"hue\": \"group\",\n",
    "            # \"hue_order\": [\"CN WMH\", \"MCI WMH\"],\n",
    "        },\n",
    "        \"annotation_func\": \"apply_test\",\n",
    "        \"configuration\": {\n",
    "            \"test\": \"Mann-Whitney\"\n",
    "        },  # this takes what normally goes into ant.configure\n",
    "        \"plot\": \"boxplot\",\n",
    "        # If you want to add the figure into a subplot...\n",
    "        #\"ax\": axs,\n",
    "    }\n",
    "\n",
    "    ant = Annotator(None, pairs)\n",
    "    # We create a FacetGrid and pass the dataframe that we want to use to later apply our functions (plotting the comparisons between groups)\n",
    "    g1 = sns.FacetGrid(df_compare_groups, aspect=1.5, height=4)\n",
    "    # map_dataframe accepts a function, which it then applies to the dataframe that is previously passed in the FacetGrid. It also accepts kwargs which\n",
    "    # are passed inside the function\n",
    "    g1.map_dataframe(ant.plot_and_annotate_facets, **kwargs)\n",
    "    #plt.close()\n",
    "\n",
    "def create_df_max_rand(df_max, df_rand_big, model_type):\n",
    "    df_max_rand = pd.DataFrame()\n",
    "\n",
    "    for row in df_max.iterrows():\n",
    "        ptid = (row[1][\"PTID\"])\n",
    "        w = (row[1][\"w\"])\n",
    "        b = (row[1][\"b\"])\n",
    "        df_max_rand = pd.concat([df_max_rand, df_rand_big[(df_rand_big[\"PTID\"] == ptid) & (df_rand_big[\"w\"] == w) & (df_rand_big[\"b\"] == b)]])\n",
    "        df_max_rand[\"Model_type\"] = model_type\n",
    "    return df_max_rand\n",
    "\n",
    "def plot_save_fig2(df_max_comp_score_a_cn_mmse, df_max_comp_score_a_mci_mmse, df_max_comp_score_G_cn_mmse, df_max_comp_score_G_mci_mmse):\n",
    "    fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize=(8, 12))\n",
    "\n",
    "    # Values that give the best composite score\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 0],\n",
    "        data=df_max_comp_score_a_cn_mmse,\n",
    "        x=\"wmh_load\",\n",
    "        y=\"a\",\n",
    "        label=\"CU WMH\",\n",
    "    )\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 0],\n",
    "        data=df_max_comp_score_a_mci_mmse,\n",
    "        x=\"wmh_load\",\n",
    "        y=\"a\",\n",
    "        label=\"MCI WMH\",\n",
    "    )\n",
    "\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 1],\n",
    "        data=df_max_comp_score_G_cn_mmse,\n",
    "        x=\"wmh_load\",\n",
    "        y=\"G\",\n",
    "        label=\"CU WMH\",\n",
    "\n",
    "    )\n",
    "    sns.regplot(\n",
    "        ax=axs[0, 1],\n",
    "        data=df_max_comp_score_G_mci_mmse,\n",
    "        x=\"wmh_load\",\n",
    "        y=\"G\",\n",
    "        label=\"MCI WMH\",\n",
    "\n",
    "    )\n",
    "\n",
    "    sns.regplot(x = \"a\", y = \"MMSE\", data = df_max_comp_score_a_cn_mmse, label = \"CU WMH\", ax = axs[1, 0])\n",
    "    sns.regplot(x = \"a\", y = \"MMSE\", data = df_max_comp_score_a_mci_mmse, label = \"MCI WMH\", ax = axs[1, 0])\n",
    "    sns.regplot(x = \"G\", y = \"MMSE\", data = df_max_comp_score_G_cn_mmse, label = \"CU WMH\", ax = axs[1, 1])\n",
    "    sns.regplot(x = \"G\", y = \"MMSE\", data = df_max_comp_score_G_mci_mmse, label = \"MCI WMH\", ax = axs[1, 1])\n",
    "\n",
    "    sns.regplot(x = \"a\", y = \"PTEDUCAT\", data = df_max_comp_score_a_cn_mmse, label = \"CU WMH\", ax = axs[2, 0])\n",
    "    sns.regplot(x = \"a\", y = \"PTEDUCAT\", data = df_max_comp_score_a_mci_mmse, label = \"MCI WMH\", ax = axs[2, 0])\n",
    "    sns.regplot(x = \"G\", y = \"PTEDUCAT\", data = df_max_comp_score_G_cn_mmse, label = \"CU WMH\", ax = axs[2, 1])\n",
    "    sns.regplot(x = \"G\", y = \"PTEDUCAT\", data = df_max_comp_score_G_mci_mmse, label = \"MCI WMH\", ax = axs[2, 1])\n",
    "\n",
    "    axs[0, 0].set_xlabel(\"Normalized WMH volume\")\n",
    "    axs[0, 1].set_xlabel(\"Normalized WMH volume\")\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 1].legend()\n",
    "    axs[1, 0].legend(loc = \"lower left\")\n",
    "    axs[1, 1].legend()\n",
    "    axs[2, 0].legend()\n",
    "    axs[2, 1].legend()\n",
    "\n",
    "\n",
    "    axs = axs.flat\n",
    "    for n, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            -0.1,\n",
    "            1.1,\n",
    "            string.ascii_uppercase[n],\n",
    "            transform=ax.transAxes,\n",
    "            size=13,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"Figure2.png\")\n",
    "    \n",
    "    # Linear fit between a, G and WMH load in CN WMH and MCI WMH\n",
    "    w_a_cn, b_a_cn = np.polyfit(df_max_comp_score_a_cn_mmse[\"wmh_load\"],  df_max_comp_score_a_cn_mmse[\"a\"], 1) \n",
    "    w_G_cn, b_G_cn = np.polyfit(df_max_comp_score_G_cn_mmse[\"wmh_load\"],  df_max_comp_score_G_cn_mmse[\"G\"], 1) \n",
    "    w_a_mci, b_a_mci = np.polyfit(df_max_comp_score_a_mci_mmse[\"wmh_load\"],  df_max_comp_score_a_mci_mmse[\"a\"], 1) \n",
    "    w_G_mci, b_G_mci = np.polyfit(df_max_comp_score_G_mci_mmse[\"wmh_load\"],  df_max_comp_score_G_mci_mmse[\"G\"], 1) \n",
    "    \n",
    "    print(\"Relationship between a and G and normalized WMH volumes\")\n",
    "    print(\"#######################################################\")\n",
    "    print(f\"In CU WMH: a = -0.02 {round(w_a_cn, 3)} * wmh {round(b_a_cn + 0.02, 2)}\")\n",
    "    print(f\"In CU WMH: G = 1.9 {round(w_G_cn, 2)} * wmh {round(b_G_cn - 1.9, 2)}\")\n",
    "    print(f\"In MCI WMH: a = -0.02 {round(w_a_mci, 3)} * wmh {round(b_a_mci + 0.02, 2)}\")\n",
    "    print(f\"In MCI WMH: G = 1.9 {round(w_G_mci, 2)} * wmh {round(b_G_mci - 1.9, 2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wmh dictionary\n",
    "wmh_dict = get_wmh_load_homogeneous(subjs_to_sim)\n",
    "# Create overall dfs for each model and populate them with single subject results\n",
    "df_a_big = pd.DataFrame()\n",
    "df_a_rand_big = pd.DataFrame()\n",
    "df_sc_big = pd.DataFrame()\n",
    "df_G_big = pd.DataFrame()\n",
    "df_G_rand_big = pd.DataFrame()\n",
    "df_het_big = pd.DataFrame()\n",
    "df_het_rand_big = pd.DataFrame()\n",
    "\n",
    "# We loop through every subject\n",
    "for subj in subjs_to_sim[:-1]:\n",
    "    # we read the csv of saved results for each subject for all models\n",
    "    df_a = pd.read_csv(\n",
    "        A_DIR / f\"sub-{subj}_df_results_homogeneous_a-weight.csv\", index_col=0\n",
    "    )\n",
    "    df_a_rand = pd.read_csv(\n",
    "        A_RAND_DIR / f\"sub-{subj}_df_results_homogeneous_a-weight.csv\",\n",
    "        index_col=0,\n",
    "    )\n",
    "    df_het = pd.read_csv(\n",
    "        HET_DIR / f\"sub-{subj}_df_results_heterogeneous.csv\",\n",
    "        index_col=0,\n",
    "    )\n",
    "    df_het_rand = pd.read_csv(\n",
    "        HET_RAND_DIR / f\"sub-{subj}_df_results_heterogeneous.csv\",\n",
    "        index_col=0,\n",
    "    )\n",
    "    df_G = pd.read_csv(\n",
    "        G_DIR / f\"sub-{subj}_df_results_homogeneous_G-weight.csv\", index_col=0\n",
    "    )\n",
    "    df_G_rand = pd.read_csv(\n",
    "        G_RAND_DIR / f\"sub-{subj}_df_results_homogeneous_G-weight.csv\", index_col=0\n",
    "    )\n",
    "    df_sc = pd.read_csv(SC_DIR / f\"sub-{subj}_df_results_disconn.csv\", index_col=0)\n",
    "\n",
    "    # Set the PTID column == to this subject\n",
    "    df_a[\"PTID\"] = subj\n",
    "    df_a_rand[\"PTID\"] = subj\n",
    "    df_het[\"PTID\"] = subj\n",
    "    df_het_rand[\"PTID\"] = subj\n",
    "    df_G[\"PTID\"] = subj\n",
    "    df_G_rand[\"PTID\"] = subj\n",
    "    df_sc[\"PTID\"] = subj\n",
    "\n",
    "    # Get the wmh load for this subject\n",
    "    df_a[\"wmh_load\"] = wmh_dict[subj]\n",
    "    df_G[\"wmh_load\"] = wmh_dict[subj]\n",
    "\n",
    "    # we concatenate results of all subjects and create model-specific datasets with all patients\n",
    "    df_a_big = pd.concat([df_a_big, df_a], ignore_index=True)\n",
    "    df_a_rand_big = pd.concat([df_a_rand_big, df_a_rand], ignore_index=True)\n",
    "    df_het_big = pd.concat([df_het_big, df_het], ignore_index=True)\n",
    "    df_het_rand_big = pd.concat([df_het_rand_big, df_het_rand], ignore_index=True)\n",
    "    df_G_big = pd.concat([df_G_big, df_G], ignore_index=True)\n",
    "    df_G_rand_big = pd.concat([df_G_rand_big, df_G_rand], ignore_index=True)\n",
    "    df_sc_big = pd.concat([df_sc_big, df_sc], ignore_index=True)\n",
    "\n",
    "# The baseline databases is with a = -0.02 and G = 1.9 == homogeneous a-weight database when b and w = 0!\n",
    "df_base = df_a_big[(df_a_big[\"w\"] == 0) & (df_a_big[\"b\"] == 0)].copy()\n",
    "df_base[\"phfcd_ks\"] = 1 - df_base[\"phfcd_ks\"] \n",
    "# Then, we convert phfcd to 1-phfcd so to have higher numbers = better fits\n",
    "df_a_big[\"phfcd_ks\"] = 1 - df_a_big[\"phfcd_ks\"]\n",
    "df_a_rand_big[\"phfcd_ks\"] = 1 - df_a_rand_big[\"phfcd_ks\"]\n",
    "df_het_big[\"phfcd_ks\"] = 1 - df_het_big[\"phfcd_ks\"]\n",
    "df_het_rand_big[\"phfcd_ks\"] = 1 - df_het_rand_big[\"phfcd_ks\"]\n",
    "df_G_big[\"phfcd_ks\"] = 1 - df_G_big[\"phfcd_ks\"]\n",
    "df_G_rand_big[\"phfcd_ks\"] = 1 - df_G_rand_big[\"phfcd_ks\"]\n",
    "df_sc_big[\"phfcd_ks\"] = 1 - df_sc_big[\"phfcd_ks\"]\n",
    "\n",
    "# Make sure that we didn't mistakenly simulate any positive weights, in case exclude them\n",
    "df_a_big = df_a_big[(df_a_big[\"b\"] <= 0) & (df_a_big[\"w\"] <= 0)].copy()\n",
    "df_a_rand_big = df_a_rand_big[(df_a_rand_big[\"b\"] <= 0) & (df_a_rand_big[\"w\"] <= 0)].copy()\n",
    "df_het_big = df_het_big[(df_het_big[\"b\"] <= 0) & (df_het_big[\"w\"] <= 0)].copy()\n",
    "df_het_rand_big = df_het_rand_big[(df_het_rand_big[\"b\"] <= 0) & (df_het_rand_big[\"w\"] <= 0)].copy()\n",
    "df_G_big = df_G_big[(df_G_big[\"b\"] <= 0) & (df_G_big[\"w\"] <= 0)].copy()\n",
    "df_G_rand_big = df_G_rand_big[(df_G_rand_big[\"b\"] <= 0) & (df_G_rand_big[\"w\"] <= 0)].copy()\n",
    "\n",
    "# In the formula for the heterogeneous model, I mistakenly performed the simulations with a - w*WMH, so to be consistent with the\n",
    "# other simulations we need to invert the weights...\n",
    "# df_het_big[\"w\"] = df_het_big[\"w\"] * -1\n",
    "# df_het_rand_big[\"w\"] = df_het_rand_big[\"w\"] * -1\n",
    "\n",
    "\n",
    "# Load the clinical dataframe with the groupings\n",
    "df_petTOAD_pre = pd.read_csv(RES_DIR / \"df_petTOAD.csv\")\n",
    "df_petTOAD_pre[\"PTID\"] = df_petTOAD_pre[\"PTID\"].str.replace(\"_\", \"\")\n",
    "df_petTOAD_pre = df_petTOAD_pre.rename(columns={\"Group_bin_Fazekas_2.0\": \"Group_Fazekas_2.0\"})\n",
    "df_petTOAD_pre['WMH_bin'] = np.where(df_petTOAD_pre[\"Group_Fazekas_2.0\"].str.contains(\"no_WMH\"), \"no_WMH\", \"WMH\")\n",
    "df_petTOAD = df_petTOAD_pre[df_petTOAD_pre[\"WMH_load_subj_space\"] < 80000]\n",
    "\n",
    "\n",
    "# Prepare databases with the maximum values for each type of model\n",
    "df_max_fc_a, df_max_phfcd_a, df_max_comp_score_a = get_best_dfs(\n",
    "    df_a_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_a_rand, df_max_phfcd_a_rand, df_max_comp_score_a_rand = get_best_dfs(\n",
    "    df_a_rand_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_hetero, df_max_phfcd_hetero, df_max_comp_score_hetero = get_best_dfs(\n",
    "    df_het_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_hetero_rand, df_max_phfcd_hetero_rand, df_max_comp_score_hetero_rand = get_best_dfs(\n",
    "    df_het_rand_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_G, df_max_phfcd_G, df_max_comp_score_G = get_best_dfs(df_G_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_G_rand, df_max_phfcd_G_rand, df_max_comp_score_G_rand = get_best_dfs(\n",
    "    df_G_rand_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")\n",
    "df_max_fc_sc, df_max_phfcd_sc, df_max_comp_score_sc = get_best_dfs(\n",
    "    df_sc_big, \"fc_pearson\", \"phfcd_ks\", \"comp_score\", wmh_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    df_a_big[\"PTID\"].unique().shape[0]\n",
    "    & df_a_rand_big[\"PTID\"].unique().shape[0]\n",
    "    & df_sc_big[\"PTID\"].unique().shape[0]\n",
    "    & df_G_big[\"PTID\"].unique().shape[0]\n",
    "    & df_G_rand_big[\"PTID\"].unique().shape[0]\n",
    "    & df_het_big[\"PTID\"].unique().shape[0]\n",
    "    & df_het_rand_big[\"PTID\"].unique().shape[0]\n",
    "    & df_base[\"PTID\"].unique().shape[0]\n",
    ") == len(subjs_to_sim[:-1]), \"Something went wrong with loading\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dfs for boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best b and w for the a-weight, G-weight and heterogeneous models\n",
    "(\n",
    "    best_fit_b_CN_a,\n",
    "    best_fit_w_CN_a,\n",
    "    best_fit_b_mci_a,\n",
    "    best_fit_w_mci_a,\n",
    ") = perform_group_analyses(df_petTOAD, df_a_big,  random=False)\n",
    "(\n",
    "    best_fit_b_CN_G,\n",
    "    best_fit_w_CN_G,\n",
    "    best_fit_b_mci_G,\n",
    "    best_fit_w_mci_G,\n",
    ") = perform_group_analyses(df_petTOAD, df_G_big, random=False)\n",
    "(\n",
    "    best_fit_b_CN_het,\n",
    "    best_fit_w_CN_het,\n",
    "    best_fit_b_mci_het,\n",
    "    best_fit_w_mci_het,\n",
    ") = perform_group_analyses(df_petTOAD, df_het_big, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random best to the same value as the non-random best\n",
    "best_fit_b_CN_a_rand, best_fit_w_CN_a_rand = best_fit_b_CN_a, best_fit_w_CN_a\n",
    "best_fit_b_CN_G_rand, best_fit_w_CN_G_rand = best_fit_b_CN_G, best_fit_w_CN_G\n",
    "best_fit_b_CN_het_rand, best_fit_w_CN_het_rand = best_fit_b_CN_het, best_fit_w_CN_het\n",
    "\n",
    "\n",
    "# We create merged databases with clinical information for the remaining models\n",
    "df_a_big_boxplot = pd.merge(\n",
    "    df_a_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_a_rand_big_boxplot = pd.merge(\n",
    "    df_a_rand_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_het_big_boxplot = pd.merge(\n",
    "    df_het_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_het_rand_big_boxplot = pd.merge(\n",
    "    df_het_rand_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_G_big_boxplot = pd.merge(\n",
    "    df_G_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_G_rand_big_boxplot = pd.merge(\n",
    "    df_G_rand_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_sc_big_boxplot = pd.merge(\n",
    "    df_sc_big, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "# Create df with only the best values from before for CN\n",
    "df_boxplot_compare_CN_base = create_df_boxplot(df_a_big_boxplot, 0, 0, \"CN_WMH\", \"Base\")\n",
    "df_boxplot_compare_CN_a = create_df_boxplot(\n",
    "    df_a_big_boxplot, best_fit_b_CN_a, best_fit_w_CN_a, \"CN_WMH\", \"Homo.\"\n",
    ")\n",
    "df_boxplot_compare_CN_a_rand = create_df_boxplot(\n",
    "    df_a_rand_big_boxplot,\n",
    "    best_fit_b_CN_a_rand,\n",
    "    best_fit_w_CN_a_rand,\n",
    "    \"CN_WMH\",\n",
    "    \"Homo. rand.\",\n",
    ")\n",
    "df_boxplot_compare_CN_het = create_df_boxplot(\n",
    "    df_het_big_boxplot, best_fit_b_CN_het, best_fit_w_CN_het, \"CN_WMH\", \"Hetero.\"\n",
    ")\n",
    "df_boxplot_compare_CN_het_rand = create_df_boxplot(\n",
    "    df_het_rand_big_boxplot,\n",
    "    best_fit_b_CN_het_rand,\n",
    "    best_fit_w_CN_het_rand,\n",
    "    \"CN_WMH\",\n",
    "    \"Hetero. rand.\",\n",
    ")\n",
    "df_boxplot_compare_CN_G = create_df_boxplot(\n",
    "    df_G_big_boxplot, best_fit_b_CN_G, best_fit_w_CN_G, \"CN_WMH\", \"G-weight\"\n",
    ")\n",
    "df_boxplot_compare_CN_G_rand = create_df_boxplot(\n",
    "    df_G_rand_big_boxplot,\n",
    "    best_fit_b_CN_G_rand,\n",
    "    best_fit_w_CN_G_rand,\n",
    "    \"CN_WMH\",\n",
    "    \"G rand.\",\n",
    ")\n",
    "df_boxplot_compare_CN_sc = df_sc_big_boxplot[\n",
    "    df_sc_big_boxplot[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "].drop(columns=\"Group_Fazekas_2.0\")\n",
    "# The structural disconnectivity model doesn't have a weight, so its processing is slightly different\n",
    "df_boxplot_compare_CN_sc[\"comp_score\"] = (\n",
    "    df_boxplot_compare_CN_sc[\"fc_pearson\"] + 1 * df_boxplot_compare_CN_sc[\"phfcd_ks\"]\n",
    ")\n",
    "df_boxplot_compare_CN_sc[\"Model_type\"] = \"Disconn.\"\n",
    "\n",
    "df_boxplot_compare_CN = pd.concat(\n",
    "    [\n",
    "        df_boxplot_compare_CN_base,\n",
    "        df_boxplot_compare_CN_a,\n",
    "        df_boxplot_compare_CN_het,\n",
    "        df_boxplot_compare_CN_G,\n",
    "        df_boxplot_compare_CN_sc,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random best to the same value as the non-random best\n",
    "best_fit_b_mci_a_rand, best_fit_w_mci_a_rand = best_fit_b_mci_a, best_fit_w_mci_a\n",
    "best_fit_b_mci_G_rand, best_fit_w_mci_G_rand = best_fit_b_mci_G, best_fit_w_mci_G\n",
    "best_fit_b_mci_het_rand, best_fit_w_mci_het_rand = (\n",
    "    best_fit_b_mci_het,\n",
    "    best_fit_w_mci_het,\n",
    ")\n",
    "\n",
    "# Perform the same steps done with CN but on MCI\n",
    "# Create df with only the best values from before for MCI\n",
    "df_boxplot_compare_mci_base = create_df_boxplot(\n",
    "    df_a_big_boxplot, 0, 0, \"MCI_WMH\", \"Base\"\n",
    ")\n",
    "df_boxplot_compare_mci_a = create_df_boxplot(\n",
    "    df_a_big_boxplot, best_fit_b_mci_a, best_fit_w_mci_a, \"MCI_WMH\", \"Homo.\"\n",
    ")\n",
    "df_boxplot_compare_mci_a_rand = create_df_boxplot(\n",
    "    df_a_rand_big_boxplot,\n",
    "    best_fit_b_mci_a_rand,\n",
    "    best_fit_w_mci_a_rand,\n",
    "    \"MCI_WMH\",\n",
    "    \"Homo. rand.\",\n",
    ")\n",
    "df_boxplot_compare_mci_het = create_df_boxplot(\n",
    "    df_het_big_boxplot, best_fit_b_mci_het, best_fit_w_mci_het, \"MCI_WMH\", \"Hetero.\"\n",
    ")\n",
    "df_boxplot_compare_mci_het_rand = create_df_boxplot(\n",
    "    df_het_rand_big_boxplot,\n",
    "    best_fit_b_mci_het_rand,\n",
    "    best_fit_w_mci_het_rand,\n",
    "    \"MCI_WMH\",\n",
    "    \"Hetero. rand.\",\n",
    ")\n",
    "df_boxplot_compare_mci_G = create_df_boxplot(\n",
    "    df_G_big_boxplot, best_fit_b_mci_G, best_fit_w_mci_G, \"MCI_WMH\", \"G-weight\"\n",
    ")\n",
    "df_boxplot_compare_mci_G_rand = create_df_boxplot(\n",
    "    df_G_rand_big_boxplot,\n",
    "    best_fit_b_mci_G_rand,\n",
    "    best_fit_w_mci_G_rand,\n",
    "    \"MCI_WMH\",\n",
    "    \"G rand.\",\n",
    ")\n",
    "df_boxplot_compare_mci_sc = df_sc_big_boxplot[\n",
    "    df_sc_big_boxplot[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "].drop(columns=\"Group_Fazekas_2.0\")\n",
    "df_boxplot_compare_mci_sc[\"comp_score\"] = (\n",
    "    df_boxplot_compare_mci_sc[\"fc_pearson\"] + 1 * df_boxplot_compare_mci_sc[\"phfcd_ks\"]\n",
    ")\n",
    "df_boxplot_compare_mci_sc[\"Model_type\"] = \"Disconn.\"\n",
    "df_boxplot_compare_mci = pd.concat(\n",
    "    [\n",
    "        df_boxplot_compare_mci_base,\n",
    "        df_boxplot_compare_mci_a,\n",
    "        df_boxplot_compare_mci_het,\n",
    "        df_boxplot_compare_mci_G,\n",
    "        df_boxplot_compare_mci_sc,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unified long df to compare the model performance between random vs. non-random\n",
    "df_boxplot_compare_rand_cn = pd.concat(\n",
    "    [\n",
    "        df_boxplot_compare_CN_a,\n",
    "        df_boxplot_compare_CN_a_rand,\n",
    "        df_boxplot_compare_CN_het,\n",
    "        df_boxplot_compare_CN_het_rand,\n",
    "        df_boxplot_compare_CN_G,\n",
    "        df_boxplot_compare_CN_G_rand,\n",
    "    ]\n",
    ")\n",
    "df_boxplot_compare_rand_cn[\"Random\"] = np.where(\n",
    "    df_boxplot_compare_rand_cn[\"Model_type\"].str.contains(\"rand.\"),\n",
    "    \"Random\",\n",
    "    \"Not Random\",\n",
    ")\n",
    "df_boxplot_compare_rand_mci = pd.concat(\n",
    "    [\n",
    "        df_boxplot_compare_mci_a,\n",
    "        df_boxplot_compare_mci_a_rand,\n",
    "        df_boxplot_compare_mci_het,\n",
    "        df_boxplot_compare_mci_het_rand,\n",
    "        df_boxplot_compare_mci_G,\n",
    "        df_boxplot_compare_mci_G_rand,\n",
    "    ]\n",
    ")\n",
    "df_boxplot_compare_rand_mci[\"Random\"] = np.where(\n",
    "    df_boxplot_compare_rand_mci[\"Model_type\"].str.contains(\"rand.\"),\n",
    "    \"Random\",\n",
    "    \"Not Random\",\n",
    ")\n",
    "# Create a unified long df to compare the model performance between groups\n",
    "df_boxplot_compare_CN[\"group\"] = \"CN WMH\"\n",
    "df_boxplot_compare_mci[\"group\"] = \"MCI WMH\"\n",
    "df_boxplot_compare = pd.concat([df_boxplot_compare_CN, df_boxplot_compare_mci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dfs with maximum values achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the best FC and 1-phFCDs achievable in the range of explored parameters\n",
    "df_base_merged = pd.merge(df_base, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\")\n",
    "df_base_merged[\"comp_score\"] = df_base_merged[\"fc_pearson\"] + 1 *  df_base_merged[\"phfcd_ks\"]\n",
    "df_max_comp_score_a_merged = pd.merge(\n",
    "    df_max_comp_score_a, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "df_max_comp_score_a_rand_merged = pd.merge(\n",
    "    df_max_comp_score_a_rand, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "df_max_comp_score_hetero_merged = pd.merge(\n",
    "    df_max_comp_score_hetero, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "df_max_comp_score_G_merged = pd.merge(\n",
    "    df_max_comp_score_G, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "df_max_comp_score_G_rand_merged = pd.merge(\n",
    "    df_max_comp_score_G_rand, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "df_max_comp_score_sc_merged = pd.merge(\n",
    "    df_max_comp_score_sc, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    ")\n",
    "\n",
    "df_base_merged[\"Model_type\"] = \"Base\"\n",
    "\n",
    "df_max_comp_score_a_merged[\"Model_type\"] = \"Homo.\"\n",
    "df_max_comp_score_a_rand_merged[\"Model_type\"] = \"Homo. rand.\"\n",
    "df_max_comp_score_hetero_merged[\"Model_type\"] = \"Hetero.\"\n",
    "df_max_comp_score_G_merged[\"Model_type\"] = \"G-weight\"\n",
    "df_max_comp_score_G_rand_merged[\"Model_type\"] = \"G rand.\"\n",
    "df_max_comp_score_sc_merged[\"Model_type\"] = \"Disconn.\"\n",
    "\n",
    "# df_max_phfcd_a_merged = pd.merge(\n",
    "#     df_max_phfcd_a, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "# df_max_phfcd_a_rand_merged = pd.merge(\n",
    "#     df_max_phfcd_a_rand, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "# df_max_phfcd_sc_merged = pd.merge(\n",
    "#     df_max_phfcd_sc, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "# df_max_phfcd_G_merged = pd.merge(\n",
    "#     df_max_phfcd_G, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "# df_max_phfcd_G_rand_merged = pd.merge(\n",
    "#     df_max_phfcd_G_rand, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "# df_max_phfcd_hetero_merged = pd.merge(\n",
    "#     df_max_phfcd_hetero, df_petTOAD[[\"PTID\", \"Group_Fazekas_2.0\"]], on=\"PTID\"\n",
    "# )\n",
    "\n",
    "# df_max_phfcd_a_merged[\"Model_type\"] = \"Homo.\"\n",
    "# df_max_phfcd_a_rand_merged[\"Model_type\"] = \"Homo. rand.\"\n",
    "# df_max_phfcd_hetero_merged[\"Model_type\"] = \"Hetero.\"\n",
    "# df_max_phfcd_G_merged[\"Model_type\"] = \"G-weight\"\n",
    "# df_max_phfcd_G_rand_merged[\"Model_type\"] = \"G rand.\"\n",
    "# df_max_phfcd_sc_merged[\"Model_type\"] = \"Disconn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only CN\n",
    "df_base_merged_CN = df_base_merged[df_base_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"]\n",
    "df_max_comp_score_a_merged_CN = df_max_comp_score_a_merged[\n",
    "    df_max_comp_score_a_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "df_max_comp_score_a_rand_merged_CN = df_max_comp_score_a_rand_merged[\n",
    "    df_max_comp_score_a_rand_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "df_max_comp_score_hetero_merged_CN = df_max_comp_score_hetero_merged[\n",
    "    df_max_comp_score_hetero_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "df_max_comp_score_G_merged_CN = df_max_comp_score_G_merged[\n",
    "    df_max_comp_score_G_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "df_max_comp_score_G_rand_merged_CN = df_max_comp_score_G_rand_merged[\n",
    "    df_max_comp_score_G_rand_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "df_max_comp_score_sc_merged_CN = df_max_comp_score_sc_merged[\n",
    "    df_max_comp_score_sc_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "]\n",
    "\n",
    "# df_max_phfcd_a_merged_CN = df_max_phfcd_a_merged[\n",
    "#     df_max_phfcd_a_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_a_rand_merged_CN = df_max_phfcd_a_rand_merged[\n",
    "#     df_max_phfcd_a_rand_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_hetero_merged_CN = df_max_phfcd_hetero_merged[\n",
    "#     df_max_phfcd_hetero_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_hetero_merged_CN = df_max_phfcd_hetero_merged_CN.dropna()\n",
    "# df_max_phfcd_G_merged_CN = df_max_phfcd_G_merged[\n",
    "#     df_max_phfcd_G_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_G_rand_merged_CN = df_max_phfcd_G_rand_merged[\n",
    "#     df_max_phfcd_G_rand_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_sc_merged_CN = df_max_phfcd_sc_merged[\n",
    "#     df_max_phfcd_sc_merged[\"Group_Fazekas_2.0\"] == \"CN_WMH\"\n",
    "# ]\n",
    "\n",
    "df_max_comp_score_CN = pd.concat(\n",
    "    [\n",
    "        df_base_merged_CN,\n",
    "        df_max_comp_score_a_merged_CN,\n",
    "        df_max_comp_score_hetero_merged_CN,\n",
    "        df_max_comp_score_G_merged_CN,\n",
    "        df_max_comp_score_sc_merged_CN,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# df_max_phfcd_CN = pd.concat(\n",
    "#     [\n",
    "#         df_base_merged_CN,\n",
    "#         df_max_phfcd_a_merged_CN,\n",
    "#         df_max_phfcd_hetero_merged_CN,\n",
    "#         df_max_phfcd_G_merged_CN,\n",
    "#         df_max_phfcd_sc_merged_CN,\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only MCI\n",
    "df_base_merged_mci = df_base_merged[df_base_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"]\n",
    "df_max_comp_score_a_merged_mci = df_max_comp_score_a_merged[\n",
    "    df_max_comp_score_a_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "df_max_comp_score_a_rand_merged_mci = df_max_comp_score_a_rand_merged[\n",
    "    df_max_comp_score_a_rand_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "df_max_comp_score_hetero_merged_mci = df_max_comp_score_hetero_merged[\n",
    "    df_max_comp_score_hetero_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "df_max_comp_score_G_merged_mci = df_max_comp_score_G_merged[\n",
    "    df_max_comp_score_G_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "df_max_comp_score_G_rand_merged_mci = df_max_comp_score_G_rand_merged[\n",
    "    df_max_comp_score_G_rand_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "df_max_comp_score_sc_merged_mci = df_max_comp_score_sc_merged[\n",
    "    df_max_comp_score_sc_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "]\n",
    "\n",
    "# df_max_phfcd_a_merged_mci = df_max_phfcd_a_merged[\n",
    "#     df_max_phfcd_a_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_a_rand_merged_mci = df_max_phfcd_a_rand_merged[\n",
    "#     df_max_phfcd_a_rand_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "# # This dataset contain multiple values for one pts, so we filter them out..\n",
    "# df_max_phfcd_a_rand_merged_mci = df_max_phfcd_a_rand_merged_mci.drop_duplicates(\n",
    "#     [\"PTID\"]\n",
    "# )\n",
    "# df_max_phfcd_hetero_merged_mci = df_max_phfcd_hetero_merged[\n",
    "#     df_max_phfcd_hetero_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_G_merged_mci = df_max_phfcd_G_merged[\n",
    "#     df_max_phfcd_G_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_G_rand_merged_mci = df_max_phfcd_G_rand_merged[\n",
    "#     df_max_phfcd_G_rand_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "# df_max_phfcd_sc_merged_mci = df_max_phfcd_sc_merged[\n",
    "#     df_max_phfcd_sc_merged[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"\n",
    "# ]\n",
    "\n",
    "df_max_comp_score_mci = pd.concat(\n",
    "    [\n",
    "        df_base_merged_mci,\n",
    "        df_max_comp_score_a_merged_mci,\n",
    "        df_max_comp_score_hetero_merged_mci,\n",
    "        df_max_comp_score_G_merged_mci,\n",
    "        df_max_comp_score_sc_merged_mci,\n",
    "    ]\n",
    ")\n",
    "# df_max_phfcd_mci = pd.concat(\n",
    "#     [\n",
    "#         df_base_merged_mci,\n",
    "#         df_max_phfcd_a_merged_mci,\n",
    "#         df_max_phfcd_hetero_merged_mci,\n",
    "#         df_max_phfcd_G_merged_mci,\n",
    "#         df_max_phfcd_sc_merged_mci,\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_comp_score_a[\"a\"] = (\n",
    "    -0.02\n",
    "    + df_max_comp_score_a[\"w\"] * df_max_comp_score_a[\"wmh_load\"]\n",
    "    + df_max_comp_score_a[\"b\"]\n",
    ")\n",
    "df_max_comp_score_G[\"G\"] = (\n",
    "    1.9\n",
    "    + df_max_comp_score_G[\"w\"] * df_max_comp_score_G[\"wmh_load\"]\n",
    "    + df_max_comp_score_G[\"b\"]\n",
    ")\n",
    "\n",
    "df_max_comp_score_a_cn = df_max_comp_score_a[df_max_comp_score_a[\"PTID\"].isin(CN_WMH)]\n",
    "df_max_comp_score_a_mci = df_max_comp_score_a[df_max_comp_score_a[\"PTID\"].isin(MCI_WMH)]\n",
    "df_max_comp_score_G_cn = df_max_comp_score_G[df_max_comp_score_G[\"PTID\"].isin(CN_WMH)]\n",
    "df_max_comp_score_G_mci = df_max_comp_score_G[df_max_comp_score_G[\"PTID\"].isin(MCI_WMH)]\n",
    "\n",
    "df_max_comp_score_a_mci_mmse = pd.merge(df_max_comp_score_a_mci, df_petTOAD, on = \"PTID\")\n",
    "df_max_comp_score_a_cn_mmse = pd.merge(df_max_comp_score_a_cn, df_petTOAD, on = \"PTID\")\n",
    "df_max_comp_score_G_mci_mmse = pd.merge(df_max_comp_score_G_mci, df_petTOAD, on = \"PTID\")\n",
    "df_max_comp_score_G_cn_mmse = pd.merge(df_max_comp_score_G_cn, df_petTOAD, on = \"PTID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_comp_score_CN[\"group\"] = \"CN WMH\"\n",
    "df_max_comp_score_mci[\"group\"] = \"MCI WMH\"\n",
    "df_max_boxplot_compare = pd.concat([df_max_comp_score_CN, df_max_comp_score_mci])\n",
    "# create tuples\n",
    "df_max_rand_compare = pd.DataFrame()\n",
    "list_tuples_df = [(df_max_comp_score_a, df_a_rand_big, \"Homo.\"), (df_max_comp_score_G, df_G_rand_big, \"G-weight\"), (df_max_comp_score_hetero, df_het_rand_big, \"Hetero.\")]\n",
    "for tp in list_tuples_df:\n",
    "    df_max_rand_compare= pd.concat([df_max_rand_compare, create_df_max_rand(tp[0], tp[1], tp[2])])\n",
    "df_max_rand_compare[\"Group_Fazekas_2.0\"] = np.where(df_max_rand_compare[\"PTID\"].isin(CN_WMH), \"CN_WMH\", \"MCI_WMH\")\n",
    "df_max_rand_compare[\"Random\"] = \"Random\"\n",
    "df_max_rand_compare[\"Model_type_composite\"] = df_max_rand_compare[\"Model_type\"] + \" \" + \"Random\"\n",
    "df_max_rand_compare_cn = df_max_rand_compare[df_max_rand_compare[\"Group_Fazekas_2.0\"] == \"CN_WMH\"]\n",
    "df_max_rand_compare_mci = df_max_rand_compare[df_max_rand_compare[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"]\n",
    "df_max_comp_score_CN[\"Random\"] = \"Not Random\"\n",
    "df_max_comp_score_mci[\"Random\"] = \"Not Random\"\n",
    "df_max_comp_score_CN[\"Model_type_composite\"] = df_max_comp_score_CN[\"Model_type\"] + df_max_comp_score_CN[\"Random\"] \n",
    "df_max_comp_score_mci[\"Model_type_composite\"] = df_max_comp_score_mci[\"Model_type\"] + df_max_comp_score_mci[\"Random\"] \n",
    "\n",
    "df_max_comp_score_boxplot_compare_rand_cn = pd.concat([df_max_rand_compare_cn, df_max_comp_score_CN])\n",
    "df_max_comp_score_boxplot_compare_rand_mci = pd.concat([df_max_rand_compare_mci, df_max_comp_score_mci])\n",
    "df_max_comp_score_boxplot_compare_rand_cn = df_max_comp_score_boxplot_compare_rand_cn[(df_max_comp_score_boxplot_compare_rand_cn[\"Model_type\"] != \"Base\") & (df_max_comp_score_boxplot_compare_rand_cn[\"Model_type\"] != \"Disconn.\")]\n",
    "df_max_comp_score_boxplot_compare_rand_mci = df_max_comp_score_boxplot_compare_rand_mci[(df_max_comp_score_boxplot_compare_rand_mci[\"Model_type\"] != \"Base\") & (df_max_comp_score_boxplot_compare_rand_mci[\"Model_type\"] != \"Disconn.\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "n_pts = len(df_petTOAD)\n",
    "age_min = round(df_petTOAD.describe()[\"Age\"][\"min\"], 0)\n",
    "age_max = round(df_petTOAD.describe()[\"Age\"][\"max\"],0)\n",
    "sex_counts = df_petTOAD['Sex'].value_counts()\n",
    "DX_counts = df_petTOAD['Group'].value_counts()\n",
    "females_num = sex_counts[\"F\"]\n",
    "males_num = sex_counts[\"M\"]\n",
    "cu_num = DX_counts['CN']\n",
    "mci_num = DX_counts['MCI']\n",
    "\n",
    "cu_no_wmh = len(df_petTOAD[df_petTOAD['Group_Fazekas_2.0'] == 'CN_no_WMH'])\n",
    "cu_wmh = len(df_petTOAD[df_petTOAD['Group_Fazekas_2.0'] == 'CN_WMH'])\n",
    "mci_no_wmh = len(df_petTOAD[df_petTOAD['Group_Fazekas_2.0'] == 'MCI_no_WMH'])\n",
    "mci_wmh = len(df_petTOAD[df_petTOAD['Group_Fazekas_2.0'] == 'MCI_WMH'])\n",
    "\n",
    "print(f\"After exclusion, {n_pts} patients (age-range: {age_min}-{age_max}, {females_num} ({round(females_num / n_pts * 100, 2)}%) females, {males_num} ({round(males_num / n_pts * 100, 2)}%) males, {cu_num} ({round(cu_num / n_pts* 100, 1)}%) CU and {mci_num} ({round(mci_num / n_pts * 100, 1)}%) MCI) were considered for the modeling pipeline.\")\n",
    "print(f\"Patients were further subdivided into subgroups according to the previously defined Fazekas score cutoff of <= 2.\")\n",
    "print(f\"Accordingly, there were: {cu_no_wmh} ({round(cu_no_wmh / cu_num * 100, 1)}%) CU without WMH, {cu_wmh} ({round(cu_wmh / cu_num * 100, 1)}%) CU with WMH, {mci_no_wmh} ({round(mci_no_wmh / mci_num * 100, 1)}%) MCI without WMH, {mci_wmh} ({round(mci_wmh / mci_num* 100, 1)}%) MCI with WMH,\")\n",
    "# Create contingency table for comparing Fazekas binary classification between groups with Chi-squared\n",
    "ct_wmh = pd.crosstab(df_petTOAD['WMH_bin'], df_petTOAD['Group'])\n",
    "_, p_wmh, _, _ = stats.chi2_contingency(ct_wmh)\n",
    "if p_wmh < 0.05:\n",
    "    print(f\"with statistically significant differences in frequency between subgroups (p = {round(p_wmh,3)})\")\n",
    "else:\n",
    "    print(f\"with no statistically significant differences in frequency between subgroups (p = {round(p_wmh, 3)})\")\n",
    "# Get age summary stats\n",
    "age_summary = df_petTOAD.groupby([\"Group\"])['Age'].describe()\n",
    "age_summary_subgroups = df_petTOAD.groupby([\"Group_Fazekas_2.0\"])['Age'].describe()\n",
    "print(\"################# Results #################\")\n",
    "print(\"# Age #\")\n",
    "pval_age_diff_cn_vs_mci = stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group\"] == \"CN\"][\"Age\"], df_petTOAD[df_petTOAD[\"Group\"] == \"MCI\"][\"Age\"])[1]\n",
    "pval_age_diff_cn_no_wmh_vs_wmh = stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"CN_WMH\"][\"Age\"], df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"CN_no_WMH\"][\"Age\"])[1]\n",
    "pval_age_diff_mci_no_wmh_vs_wmh = stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"][\"Age\"], df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"MCI_no_WMH\"][\"Age\"])[1]\n",
    "print(f\"Median age for CU subjects = {age_summary['50%']['CN']} (IQR = {age_summary['25%']['CN']} - {age_summary['75%']['CN']})\")\n",
    "print(f\"Median age for MCI subjects = {age_summary['50%']['MCI']} (IQR = {age_summary['25%']['MCI']} - {age_summary['75%']['MCI']})\")\n",
    "print(f\"P-value age CU vs MCI: {round(pval_age_diff_cn_vs_mci, 5)}\")\n",
    "\n",
    "print(f\"Median age for CU no WMH subjects = {age_summary_subgroups['50%']['CN_no_WMH']} (IQR = {age_summary_subgroups['25%']['CN_no_WMH']} - {age_summary_subgroups['75%']['CN_no_WMH']})\")\n",
    "print(f\"Median age for CU WMH subjects = {age_summary_subgroups['50%']['CN_WMH']} (IQR = {age_summary_subgroups['25%']['CN_WMH']} - {age_summary_subgroups['75%']['CN_WMH']})\")\n",
    "print(f\"P-value age CU no WMH vs CU WMH: {round(pval_age_diff_cn_no_wmh_vs_wmh, 5)}\")\n",
    "\n",
    "print(f\"Median age for MCI no WMH subjects = {age_summary_subgroups['50%']['MCI_no_WMH']} (IQR = {age_summary_subgroups['25%']['MCI_no_WMH']} - {age_summary_subgroups['75%']['MCI_no_WMH']})\")\n",
    "print(f\"Median age for MCI WMH subjects = {age_summary_subgroups['50%']['MCI_WMH']} (IQR = {age_summary_subgroups['25%']['MCI_WMH']} - {age_summary_subgroups['75%']['MCI_WMH']})\")\n",
    "print(f\"P-value age MCI no WMH vs MCI WMH: {round(pval_age_diff_mci_no_wmh_vs_wmh, 5)}\")\n",
    "\n",
    "\n",
    "print(\"# WMH #\")\n",
    "wmh_summary = df_petTOAD.groupby([\"Group\"])['WMH_load_subj_space'].describe()\n",
    "wmh_summary_subgroups = df_petTOAD.groupby([\"Group_Fazekas_2.0\"])['WMH_load_subj_space'].describe()\n",
    "\n",
    "pval_wmh_diff_cn_vs_mci= stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group\"] == \"CN\"][\"WMH_load_subj_space\"], df_petTOAD[df_petTOAD[\"Group\"] == \"MCI\"][\"WMH_load_subj_space\"])[1]\n",
    "pval_wmh_diff_cn_no_wmh_vs_wmh = stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"CN_WMH\"][\"WMH_load_subj_space\"], df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"CN_no_WMH\"][\"WMH_load_subj_space\"])[1]\n",
    "pval_wmh_diff_mci_no_wmh_vs_wmh = stats.mannwhitneyu(df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"MCI_WMH\"][\"WMH_load_subj_space\"], df_petTOAD[df_petTOAD[\"Group_Fazekas_2.0\"] == \"MCI_no_WMH\"][\"WMH_load_subj_space\"])[1]\n",
    "\n",
    "print(f\"Median WMH volume load for CN subjects = {wmh_summary['50%']['CN']} (IQR = {wmh_summary['25%']['CN']} - {wmh_summary['75%']['CN']})\")\n",
    "print(f\"Median WMH volume load for MCI subjects = {wmh_summary['50%']['MCI']} (IQR = {wmh_summary['25%']['MCI']} - {wmh_summary['75%']['MCI']})\")\n",
    "print(f\"P-value WMH volum CU vs MCI: {pval_wmh_diff_cn_vs_mci}\")\n",
    "\n",
    "print(f\"Median WMH volume load for CN no WMH subjects = {wmh_summary_subgroups['50%']['CN_no_WMH']} (IQR = {wmh_summary_subgroups['25%']['CN_no_WMH']} - {wmh_summary_subgroups['75%']['CN_no_WMH']})\")\n",
    "print(f\"Median WMH volume load for CN WMH subjects = {wmh_summary_subgroups['50%']['CN_WMH']} (IQR = {wmh_summary_subgroups['25%']['CN_WMH']} - {wmh_summary_subgroups['75%']['CN_WMH']})\")\n",
    "print(f\"P-value WMH volume CU no WMH vs CU WMH: {pval_wmh_diff_cn_no_wmh_vs_wmh}\")\n",
    "\n",
    "print(f\"Median WMH volume load for MCI no WMH subjects = {wmh_summary_subgroups['50%']['MCI_no_WMH']} (IQR = {wmh_summary_subgroups['25%']['MCI_no_WMH']} - {wmh_summary_subgroups['75%']['MCI_no_WMH']})\")\n",
    "print(f\"Median WMH volume load for MCI WMH subjects = {wmh_summary_subgroups['50%']['MCI_WMH']} (IQR = {wmh_summary_subgroups['25%']['MCI_WMH']} - {wmh_summary_subgroups['75%']['MCI_WMH']})\")\n",
    "print(f\"P-value WMH volume MCI no WMH vs MCI WMH: {pval_wmh_diff_mci_no_wmh_vs_wmh}\")\n",
    "\n",
    "print(\"# Sex #\")\n",
    "# Create contingency table for chi-squared\n",
    "ct_sex = pd.crosstab(df_petTOAD['Sex'], df_petTOAD['Group'])\n",
    "c_sex, p_sex, _, _ = stats.chi2_contingency(ct_sex)\n",
    "if p_sex < 0.05:\n",
    "    print(f\"MCI and CU showed statistically significant sex differences, p = {round(p_sex, 3)}\")\n",
    "else:\n",
    "    print(f\"MCI and CU showed no sex groups differences, p = {round(p_sex,3)}\")\n",
    "\n",
    "# Create contingency table for chi-squared\n",
    "ct_sex_wmh = pd.crosstab(df_petTOAD['Sex'], df_petTOAD['Group_Fazekas_2.0'])\n",
    "ct_sex_cn = ct_sex_wmh.iloc[:2,:2]\n",
    "c_sex_cn, p_sex_cn, _, _ = stats.chi2_contingency(ct_sex_cn)\n",
    "if p_sex_cn < 0.05:\n",
    "    print(f\"CU WMH and CU no WMH showed statistically significant sex differences, p = {round(p_sex_cn, 3)}\")\n",
    "else:\n",
    "    print(f\"CU WMH and CU no WMH showed no sex groups differences, p = {round(p_sex_cn,3)}\")\n",
    "\n",
    "# Create contingency table for chi-squared\n",
    "ct_sex_mci = ct_sex_wmh.iloc[:2,2:]\n",
    "c_sex_mci, p_sex_mci, _, _ = stats.chi2_contingency(ct_sex_mci)\n",
    "if p_sex_mci < 0.05:\n",
    "    print(f\"CU WMH and CU no WMH showed statistically significant sex differences, p = {round(p_sex_mci, 3)}\")\n",
    "else:\n",
    "    print(f\"MCI WMH and MCI no WMH showed no sex groups differences, p = {round(p_sex_mci,3)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary_data = {\n",
    "    \"Age Median (IQR)\": [\n",
    "        f\"{age_summary_subgroups['50%']['CN_no_WMH']} ({age_summary_subgroups['25%']['CN_no_WMH']} - {age_summary_subgroups['75%']['CN_no_WMH']})\",\n",
    "        f\"{age_summary_subgroups['50%']['CN_WMH']} ({age_summary_subgroups['25%']['CN_WMH']} - {age_summary_subgroups['75%']['CN_WMH']})\",\n",
    "        f\"{round(pval_age_diff_cn_no_wmh_vs_wmh, 3)}\",\n",
    "        f\"{age_summary_subgroups['50%']['MCI_no_WMH']} ({age_summary_subgroups['25%']['MCI_no_WMH']} - {age_summary_subgroups['75%']['MCI_no_WMH']})\",\n",
    "        f\"{age_summary_subgroups['50%']['MCI_WMH']} ({age_summary_subgroups['25%']['MCI_WMH']} - {age_summary_subgroups['75%']['MCI_WMH']})\",\n",
    "        f\"{round(pval_age_diff_mci_no_wmh_vs_wmh, 3)}\",\n",
    "    ],\n",
    "    \"Sex n (%)\": [\"\", \"\", round(p_sex_cn, 3), \"\", \"\", round(p_sex_mci)],\n",
    "    \"Woman\": [\n",
    "        f\"{ct_sex_cn['CN_no_WMH']['F']} ({round(ct_sex_cn['CN_no_WMH']['F'] / ct_sex_cn['CN_no_WMH'].sum() * 100, 1)}%)\",\n",
    "        f\"{ct_sex_cn['CN_WMH']['F']} ({round(ct_sex_cn['CN_WMH']['F'] / ct_sex_cn['CN_WMH'].sum() * 100, 1)}%)\",\n",
    "        \"\",\n",
    "        f\"{ct_sex_mci['MCI_no_WMH']['F']} ({round(ct_sex_mci['MCI_no_WMH']['F'] / ct_sex_mci['MCI_no_WMH'].sum() * 100, 1)}%)\",\n",
    "        f\"{ct_sex_mci['MCI_WMH']['F']} ({round(ct_sex_mci['MCI_WMH']['F'] / ct_sex_mci['MCI_WMH'].sum() * 100, 1)}%)\",\n",
    "        \"\"\n",
    "    ],\n",
    "    \"Man\": [\n",
    "        f\"{ct_sex_cn['CN_no_WMH']['M']} ({round(ct_sex_cn['CN_no_WMH']['M'] / ct_sex_cn['CN_no_WMH'].sum() * 100, 1)}%)\",\n",
    "        f\"{ct_sex_cn['CN_WMH']['M']} ({round(ct_sex_cn['CN_WMH']['M'] / ct_sex_cn['CN_WMH'].sum() * 100, 1)}%)\",\n",
    "        \"\",\n",
    "        f\"{ct_sex_mci['MCI_no_WMH']['M']} ({round(ct_sex_mci['MCI_no_WMH']['M'] / ct_sex_mci['MCI_no_WMH'].sum() * 100, 1)}%)\",\n",
    "        f\"{ct_sex_mci['MCI_WMH']['M']} ({round(ct_sex_mci['MCI_WMH']['M'] / ct_sex_mci['MCI_WMH'].sum() * 100, 1)}%)\",\n",
    "        \"\"\n",
    "    ],\n",
    "    \"WMH volume (mm^3)\": [\n",
    "        f\"{int(wmh_summary_subgroups['50%']['CN_no_WMH'])} ({int(wmh_summary_subgroups['25%']['CN_no_WMH'])}-{int(wmh_summary_subgroups['75%']['CN_no_WMH'])})\",\n",
    "        f\"{int(wmh_summary_subgroups['50%']['CN_WMH'])} ({int(wmh_summary_subgroups['25%']['CN_WMH'])}-{int(wmh_summary_subgroups['75%']['CN_WMH'])})\",\n",
    "        f\"{round(pval_wmh_diff_cn_no_wmh_vs_wmh, 5)}\",\n",
    "        f\"{int(wmh_summary_subgroups['50%']['MCI_no_WMH'])} ({int(wmh_summary_subgroups['25%']['MCI_no_WMH'])}-{int(wmh_summary_subgroups['75%']['MCI_no_WMH'])})\",\n",
    "        f\"{int(wmh_summary_subgroups['50%']['MCI_WMH'])} ({int(wmh_summary_subgroups['25%']['MCI_WMH'])}-{int(wmh_summary_subgroups['75%']['MCI_WMH'])})\",\n",
    "        f\"{round(pval_wmh_diff_mci_no_wmh_vs_wmh, 5)}\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create the summary DataFrame\n",
    "df_summary = pd.DataFrame(data=summary_data)\n",
    "\n",
    "# Print the summary DataFrame\n",
    "df_table1 = df_summary.T\n",
    "df_table1.columns = [f\"CU no WMH (n = {cu_no_wmh})\", f\"CU WMH (n = {cu_wmh})\",  \"p\", f\"MCI no WMH (n = {mci_no_wmh})\", f\"MCI WMH (n = {mci_wmh})\", \"p\"]\n",
    "df_table1.to_csv(RES_DIR / \"table_1.csv\")\n",
    "df_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary_data = {\n",
    "    \"Age Median (IQR)\": [\n",
    "        f\"{age_summary['50%']['CN']} ({age_summary['25%']['CN']} - {age_summary['75%']['CN']})\",\n",
    "        f\"{age_summary['50%']['MCI']} ({age_summary['25%']['MCI']} - {age_summary['75%']['MCI']})\",\n",
    "        f\"{round(pval_age_diff_cn_vs_mci, 4)}\"], \n",
    "    \"Sex n (%)\": [\"\", \"\", round(p_sex, 3)],\n",
    "    \"Woman\": [f\"{ct_sex.iloc[0, 0]} ({round(ct_sex.iloc[0, 0]/cu_num * 100, 1)}%)\", f\"{ct_sex.iloc[0, 1]} ({round(ct_sex.iloc[0, 1] / mci_num * 100, 1)}%)\", \"\"],\n",
    "    \"Man\": [f\"{ct_sex.iloc[1, 0]} ({round(ct_sex.iloc[1, 0]/cu_num * 100, 1)}%)\", f\"{ct_sex.iloc[1, 1]} ({round(ct_sex.iloc[1, 1] / mci_num * 100, 1)}%)\", \"\"],\n",
    "    \"WMH volume (mm^3)\": [\n",
    "        f\"{int(wmh_summary['50%']['CN'])} ({int(wmh_summary['25%']['CN'])}-{int(wmh_summary['75%']['CN'])})\",\n",
    "        f\"{int(wmh_summary['50%']['MCI'])} ({int(wmh_summary['25%']['MCI'])}-{int(wmh_summary['75%']['MCI'])})\",\n",
    "        f\"{round(pval_wmh_diff_cn_vs_mci, 3)}\"],\n",
    "    \"Fazekas score binned n (%)\": [\"\", \"\", round(p_wmh, 3)],\n",
    "    \"WMH\": [f\"{ct_wmh.iloc[0, 0]} ({round(ct_wmh.iloc[0, 0]/cu_num * 100, 1)}%)\", f\"{ct_wmh.iloc[0, 1]} ({round(ct_wmh.iloc[0, 1] / mci_num * 100, 1)}%)\", \"\"],\n",
    "    \"no WMH\": [f\"{ct_wmh.iloc[1, 0]} ({round(ct_wmh.iloc[1, 0]/cu_num * 100, 1)}%)\", f\"{ct_wmh.iloc[1, 1]} ({round(ct_wmh.iloc[1, 1] / mci_num * 100, 1)}%)\", \"\"],}\n",
    "\n",
    "# Create the summary DataFrame\n",
    "df_summary = pd.DataFrame(data = summary_data)\n",
    "\n",
    "# Print the summary DataFrame\n",
    "df_table1_bis = df_summary.T\n",
    "df_table1_bis.columns = [f\"CU, n = {cu_num}\", f\"MCI, n = {mci_num}\", \"p-value\"]\n",
    "df_table1_bis.to_csv(RES_DIR / \"table_1_group_together.csv\")\n",
    "df_table1_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "fig, axs = plt.subplots(2, 2, figsize = (10, 8))\n",
    "plot_obs(df_max_comp_score_CN, \"comp_score\", axs[0, 0])\n",
    "plot_obs(df_max_comp_score_mci, \"comp_score\", axs[0, 1]) \n",
    "axs[0, 0].set_xlabel(\"\")\n",
    "axs[0, 1].set_xlabel(\"\")\n",
    "axs[0, 0].set_ylabel(\"GFS\")\n",
    "axs[0, 1].set_ylabel(\"GFS\")\n",
    "axs[0, 0].set_title(\"CU WMH\")\n",
    "axs[0, 1].set_title(\"MCI WMH\")\n",
    "plot_random_comparison(df_max_comp_score_boxplot_compare_rand_cn, \"comp_score\", axs[1, 0])\n",
    "plot_random_comparison(df_max_comp_score_boxplot_compare_rand_mci, \"comp_score\", axs[1, 1])\n",
    "axs[1, 0].set_xlabel(\"Model type\")\n",
    "axs[1, 1].set_xlabel(\"Model type\")\n",
    "axs[1, 0].set_ylabel(\"GFS\")\n",
    "axs[1, 1].set_ylabel(\"GFS\")\n",
    "axs[1, 0].legend(loc = \"upper right\", bbox_to_anchor=(1.02, 1.175))\n",
    "axs[1, 1].legend(loc = \"upper right\", bbox_to_anchor=(1.02, 1.175))\n",
    "axs = axs.flat\n",
    "\n",
    "for n, ax in enumerate(axs):\n",
    "    \n",
    "    ax.text(-0.1, 1.1, string.ascii_uppercase[n], transform=ax.transAxes, \n",
    "            size=20, weight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CN_mci_comparison(\"comp_score\", df_max_boxplot_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save_fig2(df_max_comp_score_a_cn_mmse, df_max_comp_score_a_mci_mmse, df_max_comp_score_G_cn_mmse, df_max_comp_score_G_mci_mmse) # run these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def results_combo(df, all_pairs, rand=False):\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "    if not rand:\n",
    "        # Loop through the pairs and perform wilcoxon test to see if model performance is different among all possible pairs\n",
    "        for pair in all_pairs:\n",
    "            group1 = df[df[\"Model_type\"] == pair[0]]\n",
    "            group2 = df[df[\"Model_type\"] == pair[1]]\n",
    "            # Do it for all observables (PCC; 1-KSD; global functional score)\n",
    "            # Note: Reported as Wilcoxon T test since Scipy.stats.wilcoxon() method reports the T value and not the W value\n",
    "            t_statistic_comp_score, p_value_comp_score = stats.wilcoxon(\n",
    "                group1[\"comp_score\"], group2[\"comp_score\"]\n",
    "            )\n",
    "            # Save everything in a results dictionary\n",
    "            results[f\"{pair[0]} vs {pair[1]}\"] = {\n",
    "                \"t_statistic_comp_score\": t_statistic_comp_score,\n",
    "                \"p_value_comp_score\": p_value_comp_score,\n",
    "            }\n",
    "    # If we are dealing with random, we need to change the column that we use to get the model names, the rest stays the same\n",
    "    else:\n",
    "        for pair in all_pairs:\n",
    "            group1 = df[df[\"Model_type_composite\"] == pair[0]]\n",
    "            group2 = df[df[\"Model_type_composite\"] == pair[1]]\n",
    "            t_statistic_comp_score, p_value_comp_score = stats.wilcoxon(\n",
    "                group1[\"comp_score\"], group2[\"comp_score\"]\n",
    "            )\n",
    "            results[f\"{pair[0]} vs {pair[1]}\"] = {\n",
    "                \"t_statistic_comp_score\": t_statistic_comp_score,\n",
    "                \"p_value_comp_score\": p_value_comp_score,\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "df_max_comp_score_boxplot_compare_rand_cn = pd.concat([df_max_rand_compare_cn, df_max_comp_score_CN])\n",
    "df_max_comp_score_boxplot_compare_rand_mci = pd.concat([df_max_rand_compare_mci, df_max_comp_score_mci])\n",
    "\n",
    "# Create a column to have the composite name of the model + if it is random or non-random in the same cell\n",
    "df_max_comp_score_boxplot_compare_rand_cn[\"Model_type_composite\"] = (\n",
    "    df_max_comp_score_boxplot_compare_rand_cn[\"Model_type\"]\n",
    "    + \" \"\n",
    "    + df_max_comp_score_boxplot_compare_rand_cn[\"Random\"]\n",
    ")\n",
    "df_max_comp_score_boxplot_compare_rand_mci[\"Model_type_composite\"] = (\n",
    "    df_max_comp_score_boxplot_compare_rand_mci[\"Model_type\"]\n",
    "    + \" \"\n",
    "    + df_max_comp_score_boxplot_compare_rand_mci[\"Random\"]\n",
    ")\n",
    "# Use itertools.combinations to create a combination of all model types (same for CN and MCI)\n",
    "model_pairs = list(combinations(df_max_comp_score_boxplot_compare_rand_cn[\"Model_type\"].unique(), 2))\n",
    "# Create combination of all model types vs. their random counterparts\n",
    "random_pairs = [\n",
    "    (\"Homo. Not Random\", \"Homo. Random\"),\n",
    "    (\"Hetero. Not Random\", \"Hetero. Random\"),\n",
    "    (\"G-weight Not Random\", \"G-weight Random\"),\n",
    "]\n",
    "\n",
    "# Perform all the comparisons both for CU with WMH and for MCI with WMH for non-random vs. baseline and vs. one another...\n",
    "results_pairs_cn = results_combo(df_max_comp_score_boxplot_compare_rand_cn, model_pairs, rand=False)\n",
    "results_pairs_mci = results_combo(df_max_comp_score_boxplot_compare_rand_mci, model_pairs, rand=False)\n",
    "#... and for non-random vs. their random counterpart\n",
    "results_pairs_cn_rand = results_combo(\n",
    "    df_max_comp_score_boxplot_compare_rand_cn, random_pairs, rand=True\n",
    ")\n",
    "results_pairs_mci_rand = results_combo(\n",
    "    df_max_comp_score_boxplot_compare_rand_mci, random_pairs, rand=True\n",
    ")\n",
    "# Store everythin in dataframes \n",
    "df_results_pairs_cn = pd.DataFrame().from_dict(results_pairs_cn).T\n",
    "df_results_pairs_mci = pd.DataFrame().from_dict(results_pairs_mci).T\n",
    "df_results_pairs_cn_rand = pd.DataFrame().from_dict(results_pairs_cn_rand).T\n",
    "df_results_pairs_mci_rand = pd.DataFrame().from_dict(results_pairs_mci_rand).T\n",
    "\n",
    "# Now we create a summary dataset with quartiles and median for the different observables for CU WMH\n",
    "df_describe_model_fit_cn_wmh = round(\n",
    "    df_max_comp_score_boxplot_compare_rand_cn.groupby(\"Model_type\").describe()[\n",
    "        [\n",
    "            (\"comp_score\", \"25%\"),\n",
    "            (\"comp_score\", \"50%\"),\n",
    "            (\"comp_score\", \"75%\"),\n",
    "        ]\n",
    "    ],\n",
    "    3,\n",
    ")\n",
    "# ... and for MCI WMH\n",
    "df_describe_model_fit_mci_wmh = round(\n",
    "    df_max_comp_score_boxplot_compare_rand_mci.groupby(\"Model_type\").describe()[\n",
    "        [\n",
    "            (\"comp_score\", \"25%\"),\n",
    "            (\"comp_score\", \"50%\"),\n",
    "            (\"comp_score\", \"75%\"),\n",
    "        ]\n",
    "    ],\n",
    "    3,\n",
    ")\n",
    "\n",
    "# This is just to create a subIndex with the group name for better visualization\n",
    "df_describe_model_fit_cn_wmh_nicer = pd.concat(\n",
    "    {\"CU with WMH\": df_describe_model_fit_cn_wmh}, names=[\"Group\"]\n",
    ")\n",
    "df_describe_model_fit_mci_wmh_nicer = pd.concat(\n",
    "    {\"MCI with WMH\": df_describe_model_fit_mci_wmh}, names=[\"Group\"]\n",
    ")\n",
    "df_describe_model_fit_together = pd.concat(\n",
    "    [df_describe_model_fit_cn_wmh_nicer, df_describe_model_fit_mci_wmh_nicer]\n",
    ")\n",
    "# Here we are going to summarise in just one column median (IQR) for all observables...\n",
    "df_describe_model_fit_together[(\"GFS\", \"Median (IQR)\")] = (\n",
    "    df_describe_model_fit_together[(\"comp_score\", \"50%\")].astype(str)\n",
    "    + \" (\"\n",
    "    + df_describe_model_fit_together[(\"comp_score\", \"25%\")].astype(str)\n",
    "    + \"-\"\n",
    "    + df_describe_model_fit_together[(\"comp_score\", \"75%\")].astype(str)\n",
    "    + \")\"\n",
    ")\n",
    "# ... and we get rid of previous columns, as they are now redundant\n",
    "df_describe_model_fit_final= df_describe_model_fit_together.iloc[:, -3:]\n",
    "# Now we want to add to the table all the p-values of the comparisons between model fits that were previously calculated\n",
    "# It is a bit tedious now, as I set them one by one, probably there is a faster way?\n",
    "# We populate the table twice with the same numbers (col, row) and (row, col), but I feel it's more understandable\n",
    "# compared to just leaving a lot of empty cells...\n",
    "# We also change the names of the columns, to be in line with the observables described in the paper\n",
    "list_obs = [(\"p-value GFS\", \"p_value_comp_score\"),\n",
    "            ]\n",
    "\n",
    "# For different group we need to feed from the specific group dataset\n",
    "group_dfs = [(\"CU with WMH\", df_results_pairs_cn),\n",
    "            (\"MCI with WMH\", df_results_pairs_mci),]\n",
    "\n",
    "for obs in list_obs:\n",
    "    # We first create empty columns\n",
    "    df_describe_model_fit_final[(obs[0], \"vs. Base\")] = np.nan\n",
    "    df_describe_model_fit_final[(obs[0], \"vs. Disconn.\")] = np.nan\n",
    "    df_describe_model_fit_final[(obs[0], \"vs. G-weight\")] = np.nan\n",
    "    df_describe_model_fit_final[(obs[0], \"vs. Hetero.\")] = np.nan\n",
    "    df_describe_model_fit_final[(obs[0], \"vs. Homo.\")] = np.nan\n",
    "\n",
    "    for df in group_dfs:\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Disconn.\")], [(obs[0], \"vs. Base\")]\n",
    "        ] = df[1].loc[\"Base vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"G-weight\")], [(obs[0], \"vs. Base\")]\n",
    "        ] = df[1].loc[\"Base vs G-weight\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Hetero.\")], [(obs[0], \"vs. Base\")]\n",
    "        ] = df[1].loc[\"Base vs Hetero.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Homo.\")], [(obs[0], \"vs. Base\")]\n",
    "        ] = df[1].loc[\"Base vs Homo.\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Base\")], [(obs[0], \"vs. Disconn.\")]\n",
    "        ] = df[1].loc[\"Base vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Base\")], [(obs[0], \"vs. G-weight\")]\n",
    "        ] = df[1].loc[\"Base vs G-weight\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Base\")], [(obs[0], \"vs. Hetero.\")]\n",
    "        ] = df[1].loc[\"Base vs Hetero.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Base\")], [(obs[0], \"vs. Homo.\")]\n",
    "        ] = df[1].loc[\"Base vs Homo.\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"G-weight\")], [(obs[0], \"vs. Disconn.\")]\n",
    "        ] = df[1].loc[\"G-weight vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Hetero.\")], [(obs[0], \"vs. Disconn.\")]\n",
    "        ] = df[1].loc[\"Hetero. vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Homo.\")], [(obs[0], \"vs. Disconn.\")]\n",
    "        ] = df[1].loc[\"Homo. vs Disconn.\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Disconn.\")], [(obs[0], \"vs. G-weight\")]\n",
    "        ] = df[1].loc[\"G-weight vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Disconn.\")], [(obs[0], \"vs. Hetero.\")]\n",
    "        ] = df[1].loc[\"Hetero. vs Disconn.\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Disconn.\")], [(obs[0], \"vs. Homo.\")]\n",
    "        ] = df[1].loc[\"Homo. vs Disconn.\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Hetero.\")], [(obs[0], \"vs. G-weight\")]\n",
    "        ] = df[1].loc[\"Hetero. vs G-weight\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Homo.\")], [(obs[0], \"vs. G-weight\")]\n",
    "        ] = df[1].loc[\"Homo. vs G-weight\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"G-weight\")], [(obs[0], \"vs. Hetero.\")]\n",
    "        ] = df[1].loc[\"Hetero. vs G-weight\", obs[1]]\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"G-weight\")], [(obs[0], \"vs. Homo.\")]\n",
    "        ] = df[1].loc[\"Homo. vs G-weight\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Homo.\")], [(obs[0], \"vs. Hetero.\")]\n",
    "        ] = df[1].loc[\"Homo. vs Hetero.\", obs[1]]\n",
    "\n",
    "        df_describe_model_fit_final.loc[\n",
    "            [(df[0], \"Hetero.\")], [(obs[0], \"vs. Homo.\")]\n",
    "        ] = df[1].loc[\"Homo. vs Hetero.\", obs[1]]\n",
    "\n",
    "# We rearrange the table so that the p-values for an observable are next to the median (IQR) of that observable\n",
    "cols = df_describe_model_fit_final.columns.tolist()\n",
    "cols = cols[0:1] + cols[-15:-10] + cols[1:2] + cols[-10:-5] + cols[2:3] + cols[-5:]\n",
    "df_describe_model_fit_final = df_describe_model_fit_final[cols]\n",
    "\n",
    "# Better formatting \n",
    "all_cols = df_describe_model_fit_final.columns.to_list() \n",
    "num = df_describe_model_fit_final._get_numeric_data()\n",
    "num_cols = num.columns.to_list()\n",
    "non_num_cols = [col for col in all_cols if col not in num_cols]\n",
    "num[num.astype(float) < 0.001] = \"< 0.001\"\n",
    "df_newly_formatted = pd.concat([num, df_describe_model_fit_final[non_num_cols]], axis = 1)\n",
    "new_cols = df_newly_formatted.columns.to_list()\n",
    "new_cols_ordered = new_cols[-3:-2] + new_cols[0:5] + new_cols[-2:-1] + new_cols[5:10] + new_cols[-1:] + new_cols[10:15]\n",
    "df_newly_formatted = df_newly_formatted[new_cols_ordered]\n",
    "df_newly_formatted.to_csv(RES_DIR / \"table_model_goodness_of_fit.csv\")\n",
    "df_newly_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmh_pre = (\n",
    "        df_petTOAD_pre[\"WMH_load_subj_space\"] - df_petTOAD_pre[\"WMH_load_subj_space\"].min()\n",
    "    ) / (\n",
    "        df_petTOAD_pre[\"WMH_load_subj_space\"].max() - df_petTOAD_pre[\"WMH_load_subj_space\"].min()\n",
    "    )\n",
    "wmh_post = (\n",
    "        df_petTOAD[\"WMH_load_subj_space\"] - df_petTOAD[\"WMH_load_subj_space\"].min()\n",
    "    ) / (\n",
    "        df_petTOAD[\"WMH_load_subj_space\"].max() - df_petTOAD[\"WMH_load_subj_space\"].min()\n",
    "    )\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.hist(wmh_pre, bins = 25);\n",
    "plt.annotate(f\"Median: {round(wmh_pre.median(), 2)}\\nSkewness: {round(stats.skew(wmh_pre), 2)}\", (0.5, 50))\n",
    "plt.subplot(122)\n",
    "plt.hist(wmh_post, bins = 25);\n",
    "plt.annotate(f\"Median: {round(wmh_post.median(), 2)}\\nSkewness: {round(stats.skew(wmh_post), 2)}\", (0.5, 38))\n",
    "plt.savefig(FIG_DIR / \"distribution_pre_post_normalized_WMH.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The best (b, w) for CN subjects are as follows:\")\n",
    "# print(f\"Homogeneous a-weighted model: {best_fit_b_CN_a, best_fit_w_CN_a}\")\n",
    "# print(f\"Homogeneous G-weighted model: {best_fit_b_CN_G, best_fit_w_CN_G}\")\n",
    "# print(f\"Heterogeneous a-weighted model: {best_fit_b_CN_het, best_fit_w_CN_het}\")\n",
    "# print(\"######################################################################\")\n",
    "# print(\"The best b and w for MCI subjects are as follows:\")\n",
    "# print(f\"Homogeneous a-weighted model: {best_fit_b_mci_a, best_fit_w_mci_a}\")\n",
    "# print(f\"Homogeneous G-weighted model: {best_fit_b_mci_G, best_fit_w_mci_G}\")\n",
    "# print(f\"Heterogeneous a-weighted model: {best_fit_b_mci_het, best_fit_w_mci_het}\")\n",
    "\n",
    "# dict_summary_best_w_b = {\n",
    "#     \"\": [\"b\", \"w\", \"b\", \"w\"],\n",
    "#     \"Homogeneous a-weighted\": [\n",
    "#         f\"{best_fit_b_CN_a}\",\n",
    "#         f\"{best_fit_w_CN_a}\",\n",
    "#         f\"{best_fit_b_mci_a}\",\n",
    "#         f\"{best_fit_w_mci_a}\",\n",
    "#     ],\n",
    "#     \"G-weighted model\": [\n",
    "#         f\"{best_fit_b_CN_G}\",\n",
    "#         f\"{best_fit_w_CN_G}\",\n",
    "#         f\"{best_fit_b_mci_G}\",\n",
    "#         f\"{best_fit_w_mci_G}\",\n",
    "#     ],\n",
    "#     \"Heterogeneous a-weighted model\": [\n",
    "#         f\"{best_fit_b_CN_het}\",\n",
    "#         f\"{best_fit_w_CN_het}\",\n",
    "#         f\"{best_fit_b_mci_het}\",\n",
    "#         f\"{best_fit_w_mci_het}\",\n",
    "#     ],\n",
    "# }\n",
    "# df_summary_best_w_b = pd.DataFrame.from_dict(dict_summary_best_w_b).T\n",
    "# df_summary_best_w_b.columns = [\"CU WMH\", \"\", \"MCI WMH\", \"\"]\n",
    "# df_summary_best_w_b.to_csv(RES_DIR / \"table2.csv\")\n",
    "# df_summary_best_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the boxplot\n",
    "# fig, axs = plt.subplots(2, 2, figsize = (10, 8))\n",
    "# plot_obs(df_boxplot_compare_CN, \"comp_score\", axs[0, 0])\n",
    "# plot_obs(df_boxplot_compare_mci, \"comp_score\", axs[0, 1])\n",
    "# axs[0, 0].set_xlabel(\"\")\n",
    "# axs[0, 1].set_xlabel(\"\")\n",
    "# axs[0, 0].set_ylabel(\"GFS\")\n",
    "# axs[0, 1].set_ylabel(\"GFS\")\n",
    "# axs[0, 0].set_title(\"CU WMH\")\n",
    "# axs[0, 1].set_title(\"MCI WMH\")\n",
    "# plot_random_comparison(df_boxplot_compare_rand_cn, \"comp_score\", axs[1, 0])\n",
    "# plot_random_comparison(df_boxplot_compare_rand_mci, \"comp_score\", axs[1, 1])\n",
    "# axs[1, 0].set_xlabel(\"Model type\")\n",
    "# axs[1, 1].set_xlabel(\"Model type\")\n",
    "# axs[1, 0].set_ylabel(\"GFS\")\n",
    "# axs[1, 1].set_ylabel(\"GFS\")\n",
    "# axs[1, 0].legend(loc = \"upper right\", bbox_to_anchor=(1.02, 1.175))\n",
    "# axs[1, 1].legend(loc = \"upper right\", bbox_to_anchor=(1.02, 1.175))\n",
    "# axs = axs.flat\n",
    "\n",
    "# for n, ax in enumerate(axs):\n",
    "    \n",
    "#     ax.text(-0.1, 1.1, string.ascii_uppercase[n], transform=ax.transAxes, \n",
    "#             size=20, weight='bold')\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the model performance between groups\n",
    "# fig = plt.figure(figsize=(6,4))\n",
    "# plot_CN_mci_comparison(\"comp_score\", df_boxplot_compare)\n",
    "# plt.xlabel(\"Model type\")\n",
    "# plt.ylabel(\"GFS\")\n",
    "# plt.legend()\n",
    "# plt.savefig(FIG_DIR / \"Figure4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot a histogram depicting how the distributions of a and G vary between groups\n",
    "# list_a_homo_cn = []\n",
    "# list_G_cn = []\n",
    "# list_a_het_cn = []\n",
    "# list_wmh_cn = []\n",
    "\n",
    "# list_a_homo_mci = []\n",
    "# list_G_mci = []\n",
    "# list_a_het_mci = []\n",
    "# list_wmh_mci = []\n",
    "\n",
    "# for subj in subjs_to_sim[:-1]:\n",
    "#     wmh = wmh_dict[subj]\n",
    "#     #node_damage = get_node_damage_ext(subj)\n",
    "\n",
    "#     if subj in CN_WMH:\n",
    "#         list_wmh_cn.append(wmh)\n",
    "#         a_homo = -0.02 + best_fit_w_CN_a * wmh + best_fit_b_CN_a\n",
    "#         G = 2.0 + best_fit_w_CN_G * wmh + best_fit_b_CN_G\n",
    "#         #a_het = np.array([round(-0.02 + best_fit_w_CN_het*(1-n) + best_fit_b_CN_het, 5) for n in node_damage])\n",
    "#         list_a_homo_cn.append(a_homo)\n",
    "#         list_G_cn.append(G)\n",
    "#         #list_a_het_cn.append(a_het)\n",
    "        \n",
    "#     elif subj in MCI_WMH:\n",
    "#         list_wmh_mci.append(wmh)\n",
    "#         a_homo = -0.02 + best_fit_w_mci_a * wmh + best_fit_b_mci_a\n",
    "#         G = 2.0 + best_fit_w_mci_G * wmh + best_fit_b_mci_G\n",
    "#         #a_het = np.array([round(-0.02 + best_fit_w_mci_het*(1-n) + best_fit_b_mci_het, 5) for n in node_damage])\n",
    "#         list_a_homo_mci.append(a_homo)\n",
    "#         list_G_mci.append(G)\n",
    "#         #list_a_het_mci.append(a_het)\n",
    "\n",
    "\n",
    "# print(f\"The min. value of a for CU with WMH is {round(min(list_a_homo_cn), 3)}\")\n",
    "# print(f\"The max. value of a for CU with WMH is {round(max(list_a_homo_cn), 3)}\")\n",
    "# # print(f\"The min. value of a heterogeneous for CU with WMH is {round(min([min(node) for node in list_a_het_cn]), 3)}\")\n",
    "# # print(f\"The max. value of a heterogeneous for CU with WMH is {round(max([max(node) for node in list_a_het_cn]), 3)}\")\n",
    "# print(f\"The min. value of G for CU with WMH is {round(min(list_G_cn), 3)}\")\n",
    "# print(f\"The max. value of G for CU with WMH is {round(max(list_G_cn), 3)}\")\n",
    "\n",
    "# print(f\"The min. value of a homogeneous for MCI with WMH is {round(min(list_a_homo_mci), 3)}\")\n",
    "# print(f\"The max. value of a homogeneous for MCI with WMH is {round(max(list_a_homo_mci), 3)}\")\n",
    "# # print(f\"The min. value of a heterogeneous for MCI with WMH is {round(min([min(node) for node in list_a_het_mci]), 3)}\")\n",
    "# # print(f\"The max. value of a heterogeneous for MCI with WMH is {round(max([max(node) for node in list_a_het_mci]), 3)}\")\n",
    "# print(f\"The min. value of G for MCI with WMH is {round(min(list_G_mci), 3)}\")\n",
    "# print(f\"The max. value of G for MCI with WMH is {round(max(list_G_mci), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how the histplot changes\n",
    "    # fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    # # plt.hist(list_a_homo_cn, weights=np.ones_like(list_a_homo_cn) / len(list_a_homo_cn), alpha = 0.4, label = \"CU WMH\");\n",
    "    # axs[0, 0].hist(\n",
    "    #     list_a_homo_mci,\n",
    "    #     weights=np.ones_like(list_a_homo_mci) / len(list_a_homo_mci),\n",
    "    #     alpha=0.4,\n",
    "    #     bins=15,\n",
    "    #     label=\"MCI WMH\",\n",
    "    # )\n",
    "\n",
    "    # axs[0, 0].set_ylabel(\"Frequency\")\n",
    "    # axs[0, 0].set_xlabel(r\"$a$\")\n",
    "    # axs[0, 0].set_xlim(-0.041, -0.015)\n",
    "    # axs[0, 0].axvline(x=-0.02, color=\"orange\", lw=2, linestyle=\":\", label=\"CU no WMH\")\n",
    "    # axs[0, 0].arrow(\n",
    "    #     -0.02,\n",
    "    #     0.25,\n",
    "    #     -0.005,\n",
    "    #     0,\n",
    "    #     length_includes_head=False,\n",
    "    #     width=0.01,\n",
    "    #     head_length=0.002,\n",
    "    #     head_width=0.015,\n",
    "    #     fill=False,\n",
    "    #     linestyle=\"-\",\n",
    "    #     lw=0.8,\n",
    "    #     edgecolor=\"black\",\n",
    "    # )\n",
    "\n",
    "    # axs[0, 1].hist(\n",
    "    #     list_G_mci,\n",
    "    #     weights=np.ones_like(list_G_mci) / len(list_G_mci),\n",
    "    #     alpha=0.4,\n",
    "    #     bins=15,\n",
    "    #     label=\"MCI WMH\",\n",
    "    # )\n",
    "    # axs[0, 1].set_xlabel(r\"$G$\")\n",
    "    # axs[0, 1].set_ylabel(\"Frequency\")\n",
    "    # axs[0, 1].set_xlim(0.9, 2.1)\n",
    "    # axs[0, 1].axvline(x=2.0, color=\"orange\", lw=2, linestyle=\":\", label = \"CU WMH\")\n",
    "    # axs[0, 1].arrow(\n",
    "    #     2,\n",
    "    #     0.25,\n",
    "    #     -0.25,\n",
    "    #     0,\n",
    "    #     length_includes_head=False,\n",
    "    #     width=0.01,\n",
    "    #     head_length=0.1,\n",
    "    #     head_width=0.015,\n",
    "    #     fill=False,\n",
    "    #     linestyle=\"-\",\n",
    "    #     lw=0.8,\n",
    "    #     edgecolor=\"black\",\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCC and 1 - KSD in MCI WMH\n",
    "# fig, axs = plt.subplots(2, 2, figsize = (15, 8))\n",
    "# plot_obs(df_boxplot_compare_mci, \"fc_pearson\", axs[0, 0])\n",
    "# plot_obs(df_boxplot_compare_mci, \"phfcd_ks\", axs[0, 1])\n",
    "# plot_random_comparison(df_boxplot_compare_rand_mci, \"fc_pearson\", axs[1, 0])\n",
    "# plot_random_comparison(df_boxplot_compare_rand_mci, \"phfcd_ks\", axs[1, 1])\n",
    "\n",
    "# axs[0, 0].set_xlabel(\"\")\n",
    "# axs[0, 1].set_xlabel(\"\")\n",
    "# axs[0, 0].set_ylabel(\"PCC\")\n",
    "# axs[0, 1].set_ylabel(\"1 - KSD\")\n",
    "# axs[1, 0].set_xlabel(\"Model type\")\n",
    "# axs[1, 1].set_xlabel(\"Model type\")\n",
    "# axs[1, 0].set_ylabel(\"PCC\")\n",
    "# axs[1, 1].set_ylabel(\"1 - KSD\")\n",
    "# axs = axs.flat\n",
    "\n",
    "# for n, ax in enumerate(axs):\n",
    "    \n",
    "#     ax.text(-0.1, 1.1, string.ascii_uppercase[n], transform=ax.transAxes, \n",
    "#             size=20, weight='bold')\n",
    "# fig.tight_layout()\n",
    "# fig.suptitle(\"MCI WMH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want / need to do also PCC and 1-phFCD comparison us plt.subplot() and change the function to also accept axis\n",
    "\n",
    "# plot_CN_mci_comparison(\"fc_pearson\", axs[0])\n",
    "# axs[0].set_ylabel(\"PCC\")\n",
    "# axs[0].set_xlabel(\"Model Type\")\n",
    "# axs[0].legend([\"CU WMH\", \"MCI WMH\"])\n",
    "# leg0 = axs[0].get_legend()\n",
    "# leg0.legendHandles[1].set_color('C0')\n",
    "# leg0.legendHandles[1].set_edgecolor('k')\n",
    "# plot_CN_mci_comparison(\"phfcd_ks\", axs[1])\n",
    "# axs[1].set_ylabel(\"1 - KSD\")\n",
    "# axs[1].set_xlabel(\"Model Type\")\n",
    "# axs[1].legend([\"CU WMH\", \"MCI WMH\"])\n",
    "# leg1 = axs[1].get_legend()\n",
    "# leg1.legend_handles[1].set_color('C0')\n",
    "# leg1.legend_handles[1].set_edgecolor('k')\n",
    "# plot_CN_mci_comparison(\"comp_score\", axs[2])\n",
    "# axs[2].set_ylabel(\"GFS\")\n",
    "# axs[2].set_xlabel(\"Model Type\")\n",
    "# axs[2].legend([\"CU WMH\", \"MCI WMH\"])\n",
    "# leg2 = axs[2].get_legend()\n",
    "# leg2.legend_handles[1].set_color('C0')\n",
    "# leg2.legend_handles[1].set_edgecolor('k')\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCC and 1 - KSD in CU WMH\n",
    "# fig, axs = plt.subplots(2, 2, figsize = (20, 16))\n",
    "# plot_obs(df_boxplot_compare_CN, \"fc_pearson\", axs[0, 0])\n",
    "# plot_obs(df_boxplot_compare_CN, \"phfcd_ks\", axs[0, 1])\n",
    "# plot_random_comparison(df_boxplot_compare_rand_cn, \"fc_pearson\", axs[1, 0])\n",
    "# plot_random_comparison(df_boxplot_compare_rand_cn, \"phfcd_ks\", axs[1, 1])\n",
    "\n",
    "# axs[0, 0].set_xlabel(\"\")\n",
    "# axs[0, 1].set_xlabel(\"\")\n",
    "# axs[0, 0].set_ylabel(\"PCC\")\n",
    "# axs[0, 1].set_ylabel(\"1 - KSD\")\n",
    "# axs[1, 0].set_xlabel(\"Model type\")\n",
    "# axs[1, 1].set_xlabel(\"Model type\")\n",
    "# axs[1, 0].set_ylabel(\"PCC\")\n",
    "# axs[1, 1].set_ylabel(\"1 - KSD\")\n",
    "# axs = axs.flat\n",
    "\n",
    "# for n, ax in enumerate(axs):\n",
    "    \n",
    "#     ax.text(-0.1, 1.1, string.ascii_uppercase[n], transform=ax.transAxes, \n",
    "#             size=20, weight='bold')\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_max_comp_score_a_cn = df_max_comp_score_a[df_max_comp_score_a[\"PTID\"].isin(CN_WMH)]\n",
    "# df_max_comp_score_a_mci = df_max_comp_score_a[df_max_comp_score_a[\"PTID\"].isin(MCI_WMH)]\n",
    "\n",
    "# # Create a JointGrid for the scatter plots and histograms\n",
    "# grid1 = sns.JointGrid(height=6, ratio=3)\n",
    "\n",
    "# # Plot the scatter plots on the grid\n",
    "# sns.regplot(data=df_max_comp_score_a_cn, x=\"wmh_load\", y=\"a\", ax=grid1.ax_joint, label=\"CU WMH\")\n",
    "# sns.regplot(data=df_max_comp_score_a_mci, x=\"wmh_load\", y=\"a\", ax=grid1.ax_joint, label=\"MCI WMH\")\n",
    "\n",
    "# # Plot histograms on the top and left sides of the grid\n",
    "# sns.histplot(data=df_max_comp_score_a_cn, x=\"wmh_load\", ax=grid1.ax_marg_x, kde=True)\n",
    "# sns.histplot(data=df_max_comp_score_a_mci, x=\"wmh_load\", ax=grid1.ax_marg_x, kde=True, color = \"tab:orange\")\n",
    "# sns.histplot(data=df_max_comp_score_a_cn, y=\"a\", ax=grid1.ax_marg_y, kde=True)\n",
    "# sns.histplot(data=df_max_comp_score_a_mci, y=\"a\", ax=grid1.ax_marg_y, kde=True, color = \"tab:orange\")\n",
    "\n",
    "# # Set labels for the scatter plot and legends\n",
    "# grid1.set_axis_labels(\"WMH Load\", \"a\")\n",
    "# grid1.ax_joint.legend()\n",
    "# ax = grid1.fig.axes[0]\n",
    "# ax.text(-0.1, 1.1, \"C\", transform=ax.transAxes, \n",
    "#             size=20, weight='bold')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
