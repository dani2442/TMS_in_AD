{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409a2a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   Model simulation with neurolib   -- Version 1.0\\nLast edit:  2023/11/13\\nAuthors:    Leone, Riccardo (RL)\\nNotes:      - Running group-level simulations for CN WMH, MCI no/with WMH at a group level\\n            - Release notes:\\n                * Initial release\\nTo do:      - \\nComments:   \\n\\nSources: \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"   Group model simulation with neurolib   -- Version 1.0\n",
    "Last edit:  2023/11/13\n",
    "Authors:    Leone, Riccardo (RL)\n",
    "Notes:      - Running group-level simulations for CN WMH, MCI no/with WMH at a group level\n",
    "            - Release notes:\n",
    "                * Initial release\n",
    "To do:      - \n",
    "Comments:   \n",
    "\n",
    "Sources: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4199de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the layout...\n",
      "Done with the layout...\n",
      "The following patients were discarded for having ROIs with all zeros: []\n",
      "petTOAD Setup done!\n"
     ]
    }
   ],
   "source": [
    "# %% Initial imports\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import neurolib.utils.functions as func\n",
    "from neurolib.models.pheno_hopf import PhenoHopfModel\n",
    "from neurolib.optimize.exploration import BoxSearch\n",
    "from neurolib.utils import paths\n",
    "from neurolib.utils import pypetUtils as pu\n",
    "from neurolib.utils.parameterSpace import ParameterSpace\n",
    "\n",
    "import filteredPowerSpectralDensity as filtPowSpectr\n",
    "import my_functions as my_func\n",
    "\n",
    "from petTOAD_parameter_setup import *\n",
    "from petTOAD_setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66baadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import signal, stats\n",
    "from numba import jit\n",
    "import demean\n",
    "\n",
    "discardOffset = 0\n",
    "saveMatrix = False\n",
    "\n",
    "def tril_indices_column(N, k=0):\n",
    "    row_i, col_i = np.nonzero(\n",
    "        np.tril(np.ones(N), k=k).T)  # Matlab works in column-major order, while Numpy works in row-major.\n",
    "    Isubdiag = (col_i,\n",
    "                row_i)  # Thus, I have to do this little trick: Transpose, generate the indices, and then \"transpose\" again...\n",
    "    return Isubdiag\n",
    "\n",
    "\n",
    "def triu_indices_column(N, k=0):\n",
    "    row_i, col_i = np.nonzero(\n",
    "        np.triu(np.ones(N), k=k).T)  # Matlab works in column-major order, while Numpy works in row-major.\n",
    "    Isubdiag = (col_i,\n",
    "                row_i)  # Thus, I have to do this little trick: Transpose, generate the indices, and then \"transpose\" again...\n",
    "    return Isubdiag\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Computes the mean of the matrix\n",
    "# ==================================================================\n",
    "@jit(nopython=True)\n",
    "def mean(x, axis=None):\n",
    "    if axis == None:\n",
    "        return np.sum(x, axis) / np.prod(x.shape)\n",
    "    else:\n",
    "        return np.sum(x, axis) / x.shape[axis]\n",
    "\n",
    "@jit(nopython=True)\n",
    "def adif(a, b):\n",
    "    if np.abs(a - b) > np.pi:\n",
    "        c = 2 * np.pi - np.abs(a - b)\n",
    "    else:\n",
    "        c = np.abs(a - b)\n",
    "    return c\n",
    "\n",
    "@jit(nopython=True)\n",
    "def numba_PIM(phases, N, Tmax, dFC, PhIntMatr):\n",
    "    T = np.arange(discardOffset, Tmax - discardOffset + 1)\n",
    "    for t in T:\n",
    "        for i in range(N):\n",
    "            for j in range(i+1):\n",
    "                dFC[i, j] = np.cos(adif(phases[i, t - 1], phases[j, t - 1]))\n",
    "                dFC[j, i] = dFC[i, j]\n",
    "        PhIntMatr[t - discardOffset] = dFC\n",
    "    return PhIntMatr\n",
    "\n",
    "def calc_PIM(ts, applyFilters=False, removeStrongArtefacts=False):  # Compute the Phase-Interaction Matrix of an input BOLD signal\n",
    "    if not np.isnan(ts).any():  # No problems, go ahead!!!\n",
    "        (N, Tmax) = ts.shape\n",
    "        npattmax = Tmax - (2 * discardOffset - 1)  # calculates the size of phfcd matrix\n",
    "        # Data structures we are going to need...\n",
    "        phases = np.zeros((N, Tmax))\n",
    "        dFC = np.zeros((N, N))\n",
    "        # PhIntMatr = np.zeros((npattmax, int(N * (N - 1) / 2)))  # The int() is not needed, but... (see above)\n",
    "        PhIntMatr = np.zeros((npattmax, N, N))\n",
    "        # syncdata = np.zeros(npattmax)\n",
    "\n",
    "        # Filters seem to be always applied...\n",
    "        if applyFilters:\n",
    "            ts_filt = BOLDFilters.BandPassFilter(ts, removeStrongArtefacts=removeStrongArtefacts)  # zero phase filter the data\n",
    "        else:\n",
    "            ts_filt = ts\n",
    "\n",
    "        for n in range(N):\n",
    "            Xanalytic = signal.hilbert(demean.demean(ts_filt[n, :]))\n",
    "            phases[n, :] = np.angle(Xanalytic)\n",
    "\n",
    "        PhIntMatr = numba_PIM(phases, N, Tmax, dFC, PhIntMatr)\n",
    "\n",
    "    else:\n",
    "        warnings.warn('############ Warning!!! PhaseInteractionMatrix.from_fMRI: NAN found ############')\n",
    "        PhIntMatr = np.array([np.nan])\n",
    "    # ======== sometimes we need to plot the matrix. To simplify the code, we save it here if needed...\n",
    "    # if saveMatrix:\n",
    "    #     import scipy.io as sio\n",
    "    #     sio.savemat(save_file + '.mat', {name: PhIntMatr})\n",
    "    return PhIntMatr\n",
    "\n",
    "    \n",
    "# ==================================================================\n",
    "# numba_phFCD: convenience function to accelerate computations\n",
    "# ==================================================================\n",
    "@jit(nopython=True)\n",
    "def numba_phFCD(phIntMatr_upTri, npattmax, size_kk3):\n",
    "    phfcd = np.zeros((size_kk3))\n",
    "    kk3 = 0\n",
    "\n",
    "    for t in range(npattmax - 2):\n",
    "        p1_sum = np.sum(phIntMatr_upTri[t:t + 3, :], axis=0)\n",
    "        p1_norm = np.linalg.norm(p1_sum)\n",
    "        for t2 in range(t + 1, npattmax - 2):\n",
    "            p2_sum = np.sum(phIntMatr_upTri[t2:t2 + 3, :], axis=0)\n",
    "            p2_norm = np.linalg.norm(p2_sum)\n",
    "\n",
    "            dot_product = np.dot(p1_sum, p2_sum)\n",
    "            phfcd[kk3] = dot_product / (p1_norm * p2_norm)\n",
    "            kk3 += 1\n",
    "    return phfcd\n",
    "\n",
    "\n",
    "def phFCD(ts, applyFilters=False, removeStrongArtefacts=False):  # Compute the FCD of an input BOLD signal\n",
    "    phIntMatr = calc_PIM(ts, applyFilters=applyFilters,\n",
    "                                                 removeStrongArtefacts=removeStrongArtefacts)  # Compute the Phase-Interaction Matrix\n",
    "    if not np.isnan(phIntMatr).any():  # No problems, go ahead!!!\n",
    "        (N, Tmax) = ts.shape\n",
    "        npattmax = Tmax - (2 * discardOffset - 1)  # calculates the size of phfcd vector\n",
    "        size_kk3 = int((npattmax - 3) * (npattmax - 2) / 2)  # The int() is not needed because N*(N-1) is always even, but \"it will produce an error in the future\"...\n",
    "        Isubdiag = tril_indices_column(N, k=-1)  # Indices of triangular lower part of matrix\n",
    "        phIntMatr_upTri = np.zeros((npattmax, int(N * (N - 1) / 2)))  # The int() is not needed, but... (see above)\n",
    "        for t in range(npattmax):\n",
    "            phIntMatr_upTri[t,:] = phIntMatr[t][Isubdiag]\n",
    "        phfcd = numba_phFCD(phIntMatr_upTri, npattmax, size_kk3,)\n",
    "    else:\n",
    "        warnings.warn('############ Warning!!! phFCD.from_fMRI: NAN found ############')\n",
    "        phfcd = np.array([np.nan])\n",
    "    # if saveMatrix:\n",
    "    #     buildMatrixToSave(phfcd, npattmax - 2)\n",
    "    return phfcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d8a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_diff_group(group):\n",
    "    # Get the timeseries for the chosen group\n",
    "    group, timeseries = get_group_ts_for_freqs(group, all_fMRI_clean)\n",
    "    f_diff = filtPowSpectr.filtPowSpetraMultipleSubjects(timeseries, TR)\n",
    "    f_diff[np.where(f_diff == 0)] = np.mean(f_diff[np.where(f_diff != 0)])\n",
    "    return f_diff\n",
    "\n",
    "\n",
    "# Define the evaluation function for the subjectwise simulations\n",
    "def evaluate(traj):\n",
    "    # Get the trajectory from the search object\n",
    "    model = search.getModelFromTraj(traj)\n",
    "    # Create an empty list to store simulated BOLD\n",
    "    bold_list = []\n",
    "    # Initialize model with random initial conditions\n",
    "    model.randomICs()\n",
    "    # Run the model\n",
    "    model.run(chunkwise=True, chunksize=60000, append=True)\n",
    "    # Skip the first 120 secs to \"warm up\" the timeseries\n",
    "    # Sample every 3 secs (3000/10 of downsampling = 300)\n",
    "    ts = model.outputs.x[:, 12000::300]\n",
    "    t = model.outputs.t[12000::300]\n",
    "    # Apply the same z-score and detrending as the empirical data\n",
    "    ts_filt = BOLDFilters.BandPassFilter(ts)\n",
    "    # Append to the BOLD list\n",
    "    bold_list.append(ts_filt)\n",
    "    # We don't want the timeseries to have nans. If it has it means that the model \"exploded\"\n",
    "    if not np.isnan([np.isnan(t).any() for t in bold_list]).any():\n",
    "        print(\"No nans, good to go...\")    \n",
    "    else:\n",
    "        print(\"There are some nans, aborting...\")    \n",
    "\n",
    "    # Create and save a dictionary with the resulting processed BOLD\n",
    "    result_dict = {}\n",
    "    result_dict[\"BOLD\"] = np.array(bold_list).squeeze()\n",
    "    result_dict[\"t\"] = t\n",
    "    search.saveToPypet(result_dict, traj)\n",
    "    \n",
    "def calculate_results_from_bolds(bold_arr, n_sim, n_parms, n_nodes):\n",
    "    # Create a new array to store the FC and phFCD values for each parameter combination and simulation\n",
    "    fc_array = np.zeros([n_sim, n_parms, n_nodes, n_nodes])\n",
    "    phfcd_array = np.zeros([n_sim, n_parms, 18336])\n",
    "\n",
    "    # Iterate over each element in the bold array\n",
    "    for i in range(n_sim):\n",
    "        for j in range(n_parms):\n",
    "            print(\n",
    "                f\"Now calculating results from the {i+1}th simulation for parameter {j}...\"\n",
    "            )\n",
    "            # Get the current timeseries\n",
    "            timeseries = bold_arr[i, j].squeeze()\n",
    "\n",
    "            # Recheck the timeseries for NaNs\n",
    "            if np.isnan(timeseries).any():\n",
    "                print(\"Simulation has some nans, aborting!\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Simulation has no nans, good to go\")\n",
    "                print(\"Calculating FC..\")\n",
    "                fc_value = func.fc(timeseries)\n",
    "                print(\"Calculating phFCD\")\n",
    "                phfcd_value = my_func.phFCD(timeseries)\n",
    "                # Store the FC and phFCD value in the corresponding position in the arrays\n",
    "                fc_array[i, j] = fc_value\n",
    "                phfcd_array[i, j] = phfcd_value\n",
    "    return fc_array, phfcd_array\n",
    "\n",
    "\n",
    "def gather_results_from_repeated_simulations(\n",
    "    group, grouplist, res_dir, filename\n",
    "):\n",
    "    # We get the trajectory names in the group simulation we want\n",
    "    trajs = pu.getTrajectorynamesInFile(f\"{res_dir}/{filename}\")\n",
    "    # Create a big list to store all results\n",
    "    big_list = []\n",
    "    # For every trajectory name we load the corresponding trajectory and store the associated bold for all runs\n",
    "    for traj in trajs:\n",
    "        traj_list = []\n",
    "        tr = pu.loadPypetTrajectory(f\"{res_dir}/{filename}\", traj)\n",
    "        run_names = tr.f_get_run_names()\n",
    "        n_run = len(run_names)\n",
    "        ns = range(n_run)\n",
    "        for i in ns:\n",
    "            r = pu.getRun(i, tr)\n",
    "            traj_list.append(r[\"BOLD\"])\n",
    "        big_list.append(traj_list)\n",
    "    # Convert the big list of BOLD for every run (combination of parameters) for every trajector (number of simulations)\n",
    "    # to a numpy array\n",
    "    bold_arr = np.array(big_list)\n",
    "    n_sim = bold_arr.shape[0]\n",
    "    n_parms = bold_arr.shape[1]\n",
    "    # Calculate the arrays of FC and phFCD\n",
    "    fc_array, phfcd_array = calculate_results_from_bolds(\n",
    "        bold_arr, n_sim, n_parms, n_nodes\n",
    "    )\n",
    "    dict_group = {subj:all_fMRI_clean[subj] for subj in grouplist}\n",
    "    # Get the group-averaged empirical FC and the concatenated empirical phFCD\n",
    "    emp_fc, _, emp_phFCD = my_func.calc_and_save_group_stats(dict_group, Path(res_dir))\n",
    "    # Get the average FC across the n simulations\n",
    "    sim_fc = fc_array.mean(axis=0)\n",
    "    print(\"Calculating fcs correlations...\")\n",
    "    fc_pearson = [func.matrix_correlation(row_sim_fc, emp_fc) for row_sim_fc in sim_fc]\n",
    "    print(\"Calculating phFCDs KS distance...\")\n",
    "    phfcd_ks = [my_func.matrix_kolmogorov(phfcd_array[:, n].flatten(), emp_phFCD) for n in range(phfcd_array.shape[1])]\n",
    "    print(\"Done calculating fitting measures\")\n",
    "    return fc_pearson, phfcd_ks\n",
    "\n",
    "\n",
    "def create_df_results_het(fc_pearson, phfcd_ks, savedir, group):\n",
    "    data = [\n",
    "        [[(round(b, 3), round(w, 3)) for w in ws_het for b in bs_het], fc_pearson, phfcd_ks]\n",
    "    ]\n",
    "    columns = [\"b_w\", \"fc_pearson\", \"phfcd_ks\"]\n",
    "    res_df = pd.DataFrame(data, columns=columns).explode(columns)\n",
    "    res_df[\"b\"], res_df[\"w\"] = zip(*res_df.b_w)\n",
    "    res_df = res_df.drop(columns=[\"b_w\"])\n",
    "    res_df.to_csv(f\"{savedir}/df_results_heterogeneous_{group}.csv\")\n",
    "\n",
    "def create_df_results_sc_disconn(fc_pearson, phfcd_ks, savedir, group):\n",
    "    data = [[fc_pearson, phfcd_ks]]\n",
    "    columns = [\"fc_pearson\", \"phfcd_ks\"]\n",
    "    res_df = pd.DataFrame(data, columns=columns).explode(columns)\n",
    "    res_df.to_csv(f\"{savedir}/df_results_disconn_{group}.csv\")\n",
    "    \n",
    "def create_df_results_homo(parm_name, parms, fc_pearson, phfcd_ks, savedir, group):\n",
    "    res_df = pd.DataFrame({parm_name: parms,\n",
    "                           \"fc_pearson\": fc_pearson,\n",
    "                           \"phfcd_ks\": phfcd_ks})\n",
    "    res_df.to_csv(f\"{savedir}/df_results_{group}_{parm_name}.csv\")\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf50a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Hopf(fdiff):\n",
    "    Dmat = np.zeros_like(sc)\n",
    "    # Initialize the model (neurolib wants a Dmat to initialize the mode,\n",
    "    model = PhenoHopfModel(Cmat=sc, Dmat=Dmat)\n",
    "    # Empirical fmri is 193 timepoints at TR=3s (9.65 min) + 2 min of initial warm up of the timeseries\n",
    "    model.params[\"duration\"] = 11.65 * 60 * 1000\n",
    "    model.params[\"signalV\"] = 0\n",
    "    model.params[\"dt\"] = 0.1\n",
    "    model.params[\"sampling_dt\"] = 10.0\n",
    "    model.params[\"sigma\"] = 0.02\n",
    "    model.params[\"w\"] = 2 * np.pi * fdiff\n",
    "    return model\n",
    "\n",
    "def prepare_parms(group, model_type, G):\n",
    "    \n",
    "    if model_type == \"homogeneous_a\":\n",
    "        # Define the parametere space to explore\n",
    "        parameters = ParameterSpace(\n",
    "            {\n",
    "                \"a\":  [np.round(np.ones(n_nodes) * new_a, 3) for new_a in np.linspace(-0.1, -0.02, 81)],\n",
    "                \"K_gl\": [G],\n",
    "            },\n",
    "            kind=\"grid\",\n",
    "        )\n",
    "        filename = f\"{group}_{model_type}_model.hdf\"\n",
    "        return parameters, filename\n",
    "    \n",
    "    elif model_type == \"heterogeneous\":\n",
    "        node_damage =  get_group_node_spared(group) #now placeholder for testing, should become load_node_damage_group(group)\n",
    "        parameters = ParameterSpace(\n",
    "        {\n",
    "            \"a\": [\n",
    "                np.array([round(-0.02 + w * n + b, 5) for n in node_damage])\n",
    "                for w in ws_het\n",
    "                for b in bs_het\n",
    "            ],\n",
    "            \"K_gl\": [G],\n",
    "        },\n",
    "        kind=\"grid\",\n",
    "    )\n",
    "        filename = f\"{group}_{model_type}_model.hdf\"\n",
    "        return parameters, filename\n",
    "    \n",
    "    elif model_type == \"homogeneous_G\":\n",
    "        \n",
    "        if group == \"CN_no_WMH\":\n",
    "            # dg otherwise the rightmost value is not included, sets how the delta G you want to explore\n",
    "            # Since we explore a potentially much larger parameter space in CN_no_WMH we set the exploration\n",
    "            # \"granularity\" to  a\n",
    "            dg = 0.02\n",
    "            parameters = ParameterSpace(\n",
    "                {\n",
    "                    \"a\":  [np.ones(n_nodes) * -0.02],\n",
    "                    \"K_gl\": np.round(np.arange(0, G + dg, dg), 2),\n",
    "                },\n",
    "                kind=\"grid\",\n",
    "            )\n",
    "            filename = f\"{group}_{model_type}_model.hdf\"\n",
    "            return parameters, filename\n",
    "            \n",
    "        elif group == \"MCI_no_WMH\":\n",
    "            return\n",
    "        else:\n",
    "            # Define the parametere space to explore\n",
    "            dg = 0.01\n",
    "            parameters = ParameterSpace(\n",
    "                {\n",
    "                    \"a\":  [np.ones(n_nodes) * -0.02],\n",
    "                    \"K_gl\": np.round(np.arange(0, G + dg, dg), 2),\n",
    "                },\n",
    "                kind=\"grid\",\n",
    "            )\n",
    "            filename = f\"{group}_{model_type}_model.hdf\"\n",
    "            return parameters, filename\n",
    "\n",
    "    elif model_type == \"disconn\":\n",
    "        spared_sc = get_group_sc_wmh_weighted(group) #now placeholder for testing, should become disconn_sc = get_group_disconn(group)\n",
    "        parameters = ParameterSpace(\n",
    "        {\n",
    "            \"Cmat\": [sc * (1 - spared_sc * w) for w in ws_disconn],\n",
    "        },\n",
    "        kind=\"grid\",\n",
    "    )\n",
    "        filename = f\"{group}_{model_type}_model.hdf\"\n",
    "        return parameters, filename\n",
    "\n",
    "def create_df_results(model_type, parameters, fc_pearson, phfcd_ks, savedir, group):\n",
    "    \n",
    "    if model_type == \"homogeneous_a\":\n",
    "        parm_name = \"a\"\n",
    "        parms = parameters.a\n",
    "        res_df = create_df_results_homo(parm_name, parms, fc_pearson, phfcd_ks, savedir, group)         \n",
    "        return parm_name, res_df\n",
    "    \n",
    "    elif model_type == \"heterogeneous\":\n",
    "        create_df_results_het(fc_pearson, phfcd_ks, savedir, group)\n",
    "        return\n",
    "            \n",
    "    elif model_type == \"homogeneous_G\":\n",
    "        parm_name = \"K_gl\"\n",
    "        parms = parameters.K_gl\n",
    "        res_df = create_df_results_homo(parm_name, parms, fc_pearson, phfcd_ks, savedir, group)\n",
    "        if group == \"CN_no_WMH\":\n",
    "            best_G = res_df[res_df[\"phfcd_ks\"] == res_df[\"phfcd_ks\"].min()][\"K_gl\"]\n",
    "            best_G.to_csv(SIM_DIR / \"best_G_CN_no_WMH\")\n",
    "        return parm_name, res_df\n",
    "\n",
    "    elif model_type == \"disconn\":\n",
    "        create_df_results_sc_disconn(savedir, fc_pearson, phfcd_ks, group)\n",
    "        return\n",
    "\n",
    "def save_plot_results_fitting_parm(parm_name, res_df, group):\n",
    "    dict_x_lab = {\"a\": \"Bifurcation parameter (a)\",\n",
    "                \"K_gl\": \"Global coupling (G)\"}\n",
    "    if parm_name == \"a\":\n",
    "        x = [res_df[\"a\"][i][0] for i in range(res_df.shape[0])]\n",
    "    else:\n",
    "        x = res_df[parm_name]\n",
    "    plt.figure()\n",
    "    plt.plot(x, res_df[\"fc_pearson\"], label=\"FC\")\n",
    "    plt.plot(x, res_df[\"phfcd_ks\"], label=\"phFCD\")\n",
    "    plt.xlabel(dict_x_lab[parm_name])\n",
    "    plt.ylabel(r\"Pearson's $\\rho$ / KS-distance\")\n",
    "    plt.legend()\n",
    "    savedir = FIG_DIR / group\n",
    "    if not Path.exists(savedir):\n",
    "        Path.mkdir(savedir)\n",
    "    plt.savefig(savedir/ f\"{group}_{parm_name}_fitting.png\")\n",
    "\n",
    "def simulate_group(\n",
    "    group, grouplist, fdiff, G, nsim, model_type\n",
    "):\n",
    "    global search\n",
    "    # Set the directory where to save results\n",
    "    savedir = str(SIM_DIR / group)\n",
    "    paths.HDF_DIR = savedir\n",
    "    print(\n",
    "        f\"Now performing the simulations for the {group} group, model: {model_type}\"\n",
    "    )\n",
    "    model = initialize_Hopf(fdiff)\n",
    "    parameters, filename = prepare_parms(group, model_type, G)\n",
    "    for i in range(nsim):\n",
    "        print(f\"Starting simulations n°: {i+1}/{nsim}\")\n",
    "        # Initialize the search\n",
    "        search = BoxSearch(\n",
    "            model=model,\n",
    "            evalFunction=evaluate,\n",
    "            parameterSpace=parameters,\n",
    "            filename=filename,\n",
    "        )\n",
    "        search.run(chunkwise=True, chunksize=60000, append=True)\n",
    "    fc_pearson, phfcd_ks = gather_results_from_repeated_simulations(\n",
    "        group, grouplist, savedir, filename,\n",
    "    )\n",
    "    if model_type not in [\"heterogeneous\", \"disconn\"]:\n",
    "        parm_name, res_df = create_df_results(model_type, parameters, fc_pearson, phfcd_ks, savedir, group)\n",
    "        save_plot_results_fitting_parm(parm_name, res_df, group)\n",
    "        return res_df\n",
    "    else:\n",
    "        create_df_results(model_type, parameters, fc_pearson, phfcd_ks, savedir, group)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99, 1.  ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_min_disconn = 0.0\n",
    "ws_max_disconn = 1.0\n",
    "# Set the number of parameters you want your min-max interval to be split into \n",
    "n_ws_disconn = 101\n",
    "# Create the final array with all the ws and bs you want to explore\n",
    "ws_disconn = np.linspace(ws_min_disconn, ws_max_disconn, n_ws_disconn)\n",
    "ws_disconn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aaed2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtPowSpectraMultipleSubjects: subject 1 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 2 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 3 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 4 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 5 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 6 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 7 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 8 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 9 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 10 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 11 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 12 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 13 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 14 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 15 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 16 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 17 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 18 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 19 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 20 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 21 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 22 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 23 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 24 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 25 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 26 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 27 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 28 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 29 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 30 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 31 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 32 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 33 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 34 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 35 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 36 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 37 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 38 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 39 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 40 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 41 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 42 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 43 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 44 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 45 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 46 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 47 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 48 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 49 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 50 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 51 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 52 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 53 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 54 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 55 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 56 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 57 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 58 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 59 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 60 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 61 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 62 (of 63)\n",
      "filtPowSpectraMultipleSubjects: subject 63 (of 63)\n",
      "Now performing the simulations for the CN_no_WMH group, model: homogeneous_G\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "/home/leoner/petTOAD/results/final_simulations/CN_no_WMH/CN_no_WMH_homogeneous_G_model.hdf does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     G \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(SIM_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_G_CN_no_WMH\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK_gl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()    \n\u001b[0;32m---> 20\u001b[0m \u001b[43msimulate_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdiff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 154\u001b[0m, in \u001b[0;36msimulate_group\u001b[0;34m(group, grouplist, fdiff, G, nsim, model_type)\u001b[0m\n\u001b[1;32m    143\u001b[0m     parameters, filename \u001b[38;5;241m=\u001b[39m prepare_parms(group, model_type, G)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#     for i in range(nsim):\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#         print(f\"Starting simulations n°: {i+1}/{nsim}\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#         # Initialize the search\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m#         search.run(chunkwise=True, chunksize=60000, append=True)\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     fc_pearson, phfcd_ks \u001b[38;5;241m=\u001b[39m \u001b[43mgather_results_from_repeated_simulations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouplist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavedir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheterogeneous\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisconn\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    158\u001b[0m         parm_name, res_df \u001b[38;5;241m=\u001b[39m create_df_results(model_type, parameters, fc_pearson, phfcd_ks, savedir, group)\n",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m, in \u001b[0;36mgather_results_from_repeated_simulations\u001b[0;34m(group, grouplist, res_dir, filename)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather_results_from_repeated_simulations\u001b[39m(\n\u001b[1;32m     70\u001b[0m     group, grouplist, res_dir, filename\n\u001b[1;32m     71\u001b[0m ):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# We get the trajectory names in the group simulation we want\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     trajs \u001b[38;5;241m=\u001b[39m \u001b[43mpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTrajectorynamesInFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mres_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Create a big list to store all results\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     big_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/neurolib/lib/python3.10/site-packages/neurolib/utils/pypetUtils.py:20\u001b[0m, in \u001b[0;36mgetTrajectorynamesInFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetTrajectorynamesInFile\u001b[39m(filename):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Return a list of all pypet trajectory names in a a given hdf5 file.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    :rtype: list[str]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pathlib\u001b[38;5;241m.\u001b[39mPath(filename)\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     hdf \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     all_traj_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(hdf\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mAssertionError\u001b[0m: /home/leoner/petTOAD/results/final_simulations/CN_no_WMH/CN_no_WMH_homogeneous_G_model.hdf does not exist!"
     ]
    }
   ],
   "source": [
    "group_dict = {\"CN_no_WMH\": CN_no_WMH, \"MCI_no_WMH\": MCI_no_WMH, \"CN_WMH\": CN_WMH, \"MCI_WMH\": MCI_WMH}\n",
    "model_types_per_group = {\n",
    "#\"CN_no_WMH\": [\"homogeneous_G\"],\n",
    "#\"MCI_no_WMH\": [\"homogeneous_a\"],\n",
    "\"CN_WMH\": [\"disconn\"], #[\"homogeneous_a\", \"homogeneous_G\", \"heterogeneous\", \"disconn\"],\n",
    "\"MCI_WMH\": [\"disconn\"] #[\"homogeneous_a\", \"homogeneous_G\", \"heterogeneous\", \"disconn\"],\n",
    "}\n",
    "\n",
    "\n",
    "G = 3.5\n",
    "nsim=1\n",
    "\n",
    "for group_name, group_list in group_dict.items():\n",
    "    for model_type in model_types_per_group[group_name]:\n",
    "        fdiff = get_f_diff_group(group_list)\n",
    "        if group_name == \"CN_no_WMH\":\n",
    "            G = 3.5\n",
    "        else:\n",
    "            G = pd.read_csv(SIM_DIR / \"best_G_CN_no_WMH\")[\"K_gl\"].to_list()    \n",
    "        simulate_group(\n",
    "                group_name, group_list, fdiff, G, nsim, model_type\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################################\n",
    "# #### Define the subject on which to perform the simulation ####\n",
    "# ###############################################################\n",
    "# id_subj = int(sys.argv[1]) - 1\n",
    "# subj = subjs_to_sim[id_subj]\n",
    "# n_subjs = len(subjs_to_sim)\n",
    "# n_sim = 20\n",
    "# best_G = 1.9\n",
    "\n",
    "# ################################################################\n",
    "# # Perform subject-wise simulations\n",
    "# ################################################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     # %% Get the frequencies for each group\n",
    "#     f_diff_CN_no_wmh = get_f_diff_group(CN_no_WMH)\n",
    "#     f_diff_CN_WMH = get_f_diff_group(CN_WMH)\n",
    "#     f_diff_MCI_no_WMH = get_f_diff_group(MCI_no_WMH)\n",
    "#     f_diff_MCI_WMH = get_f_diff_group(MCI_WMH)\n",
    "\n",
    "\n",
    "#     print(f\"SLURM ARRAY TASK: {id_subj} corresponds to subject {subj}\")\n",
    "#     print(f\"We are going to do {n_sim} simulations for subject {subj}...\")\n",
    "\n",
    "#     random_conditions = [False, True]\n",
    "\n",
    "#     for random_value in random_conditions:\n",
    "#         if not random_value:\n",
    "#             SIM_DIR_A = (\n",
    "#                 SIM_DIR / f\"a-weight_ws_{ws_min_a}-{ws_max_a}_bs_{bs_min_a}-{bs_max_a}\"\n",
    "#             )\n",
    "#             SIM_DIR_G = (\n",
    "#                 SIM_DIR / f\"G-weight_ws_{ws_min_G}-{ws_max_G}_bs_{bs_min_G}-{bs_max_G}\"\n",
    "#             )\n",
    "#             SIM_DIR_SC = SIM_DIR / f\"sc_disconn\"\n",
    "#             SIM_DIR_HET = (\n",
    "#                 SIM_DIR\n",
    "#                 / f\"heterogeneous_ws_{ws_min_het}-{ws_max_het}_bs_{bs_min_het}-{bs_max_het}\"\n",
    "#             )\n",
    "#             wmh_dict = get_wmh_load_homogeneous(subjs)\n",
    "#         else:\n",
    "#             SIM_DIR_A = (\n",
    "#                 SIM_DIR\n",
    "#                 / f\"a-weight_ws_{ws_min_a}-{ws_max_a}_bs_{bs_min_a}-{bs_max_a}_random\"\n",
    "#             )\n",
    "#             SIM_DIR_G = (\n",
    "#                 SIM_DIR\n",
    "#                 / f\"G-weight_ws_{ws_min_G}-{ws_max_G}_bs_{bs_min_G}-{bs_max_G}_random\"\n",
    "#             )\n",
    "#             SIM_DIR_SC = SIM_DIR / f\"sc_disconn\"\n",
    "#             SIM_DIR_HET = (\n",
    "#                 SIM_DIR\n",
    "#                 / f\"heterogeneous_ws_{ws_min_het}-{ws_max_het}_bs_{bs_min_het}-{bs_max_het}_random\"\n",
    "#             )\n",
    "#             wmh_dict = get_wmh_load_random(subjs)\n",
    "\n",
    "#         sim_dir = [SIM_DIR_A, SIM_DIR_G, SIM_DIR_HET, SIM_DIR_SC]\n",
    "#         for path in sim_dir:\n",
    "#             if not Path.exists(path):\n",
    "#                 Path.mkdir(path)\n",
    "\n",
    "#         if subj in CN_WMH:\n",
    "#             f_diff = f_diff_CN_WMH\n",
    "#         elif subj in MCI_WMH:\n",
    "#             f_diff = f_diff_MCI_WMH\n",
    "\n",
    "#         simulate_homogeneous_model_a(\n",
    "#             subj=subj,\n",
    "#             f_diff=f_diff,\n",
    "#             best_G=best_G,\n",
    "#             wmh_dict=wmh_dict,\n",
    "#             ws=ws_a,\n",
    "#             bs=bs_a,\n",
    "#             random_cond=random_value,\n",
    "#             sim_dir=SIM_DIR_A,\n",
    "#             nsim=n_sim,\n",
    "#         )\n",
    "#         simulate_homogeneous_model_G(\n",
    "#             subj=subj,\n",
    "#             f_diff=f_diff,\n",
    "#             wmh_dict=wmh_dict,\n",
    "#             ws=ws_G,\n",
    "#             bs=bs_G,\n",
    "#             random_cond=random_value,\n",
    "#             sim_dir=SIM_DIR_G,\n",
    "#             nsim=n_sim,\n",
    "#         )\n",
    "\n",
    "#         simulate_heterogeneous_model(\n",
    "#             subj=subj,\n",
    "#             cn_wmh_arr=CN_WMH,\n",
    "#             mci_wmh_arr=MCI_WMH,\n",
    "#             f_diff=f_diff,\n",
    "#             best_G=best_G,\n",
    "#             ws=ws_het,\n",
    "#             bs=bs_het,\n",
    "#             random_cond=random_value,\n",
    "#             sim_dir=SIM_DIR_HET,\n",
    "#             nsim=n_sim,\n",
    "#         )\n",
    "#         simulate_disconn_model(\n",
    "#             subj=subj,\n",
    "#             best_G=best_G,\n",
    "#             f_diff=f_diff,\n",
    "#             random_cond=random_value,\n",
    "#             sim_dir=SIM_DIR_SC,\n",
    "#             nsim=n_sim,\n",
    "#         )\n",
    "#         print(\"The end.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd60e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
