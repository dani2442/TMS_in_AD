{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that all the code is basically taken from Gustavo Patow's WholeBrain module (https://github.com/dagush/WholeBrain)\n",
    "\n",
    "# Load all the packages needed for analyses\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nilearn\n",
    "\n",
    "# Set up Hopf as our model \n",
    "import WholeBrain.Models.supHopf as Hopf\n",
    "from WholeBrain.simulate_SimOnly import Tmaxneuronal\n",
    "Hopf.initialValue = 0.1\n",
    "neuronalModel = Hopf\n",
    "\n",
    "# Set up our integrator\n",
    "import WholeBrain.Integrator_EulerMaruyama as myIntegrator\n",
    "integrator = myIntegrator\n",
    "integrator.neuronalModel = neuronalModel\n",
    "integrator.verbose = False\n",
    "integrator.clamping = False\n",
    "\n",
    "# Set up the integration parameters\n",
    "dt = 5e-5\n",
    "# tmax is equal to the number of timepoints: 193\n",
    "tmax= 193\n",
    "ds = 1e-4\n",
    "Tmaxneuronal = int((tmax+dt))\n",
    "\n",
    "import WholeBrain.simulate_SimOnly as simulateBOLD\n",
    "simulateBOLD.warmUp = True\n",
    "simulateBOLD.integrator = integrator\n",
    "simulateBOLD.warmUpFactor = 606./2000.\n",
    "\n",
    "# Set up the code to obtain the variables we want to maximize similarity to empirical FC\n",
    "import WholeBrain.Observables.FC as FC\n",
    "import WholeBrain.Observables.phFCD as phFCD\n",
    "import WholeBrain.Optimizers.ParmSeep as ParmSeep\n",
    "ParmSeep.simulateBOLD = simulateBOLD\n",
    "ParmSeep.integrator = integrator\n",
    "ParmSeep.verbose = True\n",
    "\n",
    "# set BOLD filter settings\n",
    "import WholeBrain.Utils.filteredPowerSpectralDensity as filtPowSpectr\n",
    "import WholeBrain.BOLDFilters as BOLDfilters\n",
    "\n",
    "# These filters are applied in the filtPowSpectr function that we use to extract the intrinsic frequencies of each region.\n",
    "# They are also applied to process the FC and swFCD and phFCD, but you can set the corresponding parameter to False later on. 0.04-0.07 Hz common to extract intrinsic frequencies\n",
    "BOLDfilters.flp = 0.04\n",
    "BOLDfilters.fhi = 0.07\n",
    "BOLDfilters.TR = 3.0\n",
    "\n",
    "# Get the list of names of all regions in the AAL atlas. This is needed to get the right indices, to then filter the FC\n",
    "import csv\n",
    "# This is a sublist of labels of the cortical regions that were included in the paper by Demirtas et al. - AAL atlas (78 regions, excluding infratentorial and deep)\n",
    "with open ('/home/riccardo/ADNI_Hopf/Utils/aal_regions_included.csv', newline='') as f:\n",
    "    new_reader = csv.reader(f)\n",
    "    included_regions = list(new_reader)\n",
    "f.close()\n",
    "\n",
    "# Get the AAL atlas labels\n",
    "import nilearn.datasets as datasets\n",
    "aal = datasets.fetch_atlas_aal()\n",
    "labels = np.array(aal.labels)\n",
    "# create an array with the indices of each label (note that these are not the label number from the nifti image)\n",
    "indices = np.array([i for i in enumerate(labels)])\n",
    "SC_regions_index = np.isin(labels, included_regions)\n",
    "# filter the indices that we want based on the position so to have a final SC matrix only for the regions we considered.\n",
    "SC_78_regions_aal_atlas = indices[SC_regions_index]\n",
    "filter_SC = np.array([int(i) for i in SC_78_regions_aal_atlas[:,0]])\n",
    "\n",
    "# Set file path for SC matrix\n",
    "x_path = '/home/riccardo/ADNI_Hopf/Utils/'\n",
    "# Load structural connectivity matrix and use it as parameter in Hopf model\n",
    "xfile = 'SCmatrices88healthy.mat' \n",
    "M = sio.loadmat(x_path + xfile); \n",
    "mat = M['SCmatrices']\n",
    "# averaging the SC among subjects\n",
    "mat0 = np.mean(mat,axis = 0)\n",
    "# Filter the SC to have just the 78 regions we considered\n",
    "x_mat0 = mat0[filter_SC]\n",
    "new_mat0 = x_mat0.T[filter_SC]\n",
    "# Prevent full synchronization of the model\n",
    "SCnorm = new_mat0 * 0.2 / new_mat0.max() \n",
    "np.fill_diagonal(SCnorm,0)\n",
    "print('SCnorm.shape={}'.format(new_mat0.shape))    \n",
    "Hopf.setParms({'SC':SCnorm})\n",
    "\n",
    "# Set the base_a_value \n",
    "base_a_value = -0.027\n",
    "# We want to warmup the timeseries before modeling\n",
    "warmUp = True\n",
    "warmUpFactor = 10.\n",
    "\n",
    "# Set up useful functions that we will use later\n",
    "\n",
    "def computeSubjectSimulation():\n",
    "    # integrator.neuronalModel.SC = C\n",
    "    # integrator.initBookkeeping(N, Tmaxneuronal)\n",
    "    if warmUp:\n",
    "        currObsVars = integrator.warmUpAndSimulate(dt, Tmaxneuronal, TWarmUp=Tmaxneuronal/warmUpFactor)\n",
    "    else:\n",
    "        currObsVars = integrator.simulate(dt, Tmaxneuronal)\n",
    "    # currObsVars = integrator.returnBookkeeping()  # curr_xn, curr_rn\n",
    "    neuro_act = currObsVars[:,1,:]  # curr_rn\n",
    "    return neuro_act\n",
    "    \n",
    "\n",
    "def loadXBurden(dict_type):\n",
    "\n",
    "    # ------------------- load and stack the different wm burdens\n",
    "    if dict_type == 'global':\n",
    "        homo_wm_file = open('/home/riccardo/ADNI_Hopf/Results/overall_WMH_burden_all_normalized.pkl','rb')\n",
    "        dict_wm_overall = pickle.load(homo_wm_file)\n",
    "    \n",
    "    elif dict_type == 'regional':\n",
    "        hetero_wm_file = open('/home/riccardo/ADNI_Hopf/Results/normalized_WMH_lobewise_all.pkl','rb')\n",
    "        dict_wm_overall = pickle.load(hetero_wm_file)\n",
    "            \n",
    "    # wm_hc = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_HC.npy')\n",
    "    # wm_mci = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_MCI.npy')\n",
    "    # wm_overall = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_ALL.npy')\n",
    "    # ------------------- load the specific subject wm\n",
    "\n",
    "    # ------------------- normalize and return\n",
    "    # wmBurdenNorm = (wmBurden - np.min(wmBurden))/np.ptp(wmBurden)  # Normalize each individual in [0,1]\n",
    "    #wmBurdenNorm = (wmBurden - np.min(wm_overall))/np.ptp(wm_overall)  # Normalize the whole group in [0,1]\n",
    "    return dict_wm_overall\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Retrieve the data for all subjects \n",
    "# ------------------------------------------------\n",
    "conditionToStudy = 'mci' # one of 'hc', 'mci', 'all'\n",
    "mode = 'baseline' # one of 'homogeneous', 'heterogeneous', 'baseline'\n",
    "random = False # set to true if you want to shuffle the wmh weights\n",
    "\n",
    "# Probably all of this could be done in a different script....\n",
    "# Load the timeseries for all subjects\n",
    "all_timeseries = np.load('/home/riccardo/ADNI_Hopf/Results/timeseries_all.npy')\n",
    "# Get the number of nodes and the Tmax\n",
    "nNodes, Tmax = all_timeseries.shape[1:]\n",
    "# Load the subject names for all subjects\n",
    "all_subjects_names = np.load('/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_all.npy')\n",
    "# Load the overall normalized WMH burden for each subject\n",
    "global_wm_overall = loadXBurden('global')\n",
    "# Load the normalized regional WMH burden for each subject\n",
    "regional_wm_overall = loadXBurden('regional')\n",
    "# create a unified dictionary to make sure we don't make mistakes when filtering\n",
    "all_dictionary = {k:{'timeseries': all_timeseries[n],\n",
    "                     'total_WMH_load': global_wm_overall[k],\n",
    "                     'regional_WMH_load': regional_wm_overall[k]} for n, k in np.ndenumerate(all_subjects_names)}\n",
    "\n",
    "if conditionToStudy == 'hc':\n",
    "\n",
    "    #load the names of HC\n",
    "    subjects_names = np.load('/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_HC.npy')\n",
    "    # filter the unified dictionary to retain just HC\n",
    "    all_fMRI = {k:v['timeseries'] for k, v in all_dictionary.items() if k in subjects_names}\n",
    "    nsubjects = len(all_fMRI) \n",
    "\n",
    "    if mode == 'homogeneous':\n",
    "        wmBurden_dict = {k:v['total_WMH_load'] for k, v in all_dictionary.items() if k in subjects_names}\n",
    "        wmBurden = np.array([v for v in wmBurden_dict.values()])\n",
    "\n",
    "    elif mode == 'heterogeneous':\n",
    "        wmBurden_dict ={k:v['regional_WMH_load'] for k, v in all_dictionary.items() if k in subjects_names} \n",
    "        wmBurden = np.array([v for v in wmBurden_dict.values()])\n",
    "    else:\n",
    "        filter_HC = np.load('/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/subject_list_wmh_volumes_HC_minimal.npy', allow_pickle = True).flatten()\n",
    "        all_fMRI = {k:v for k, v in all_fMRI.items() if k not in filter_HC}\n",
    "\n",
    "elif conditionToStudy == 'mci':\n",
    "\n",
    "    #load the names of MCI\n",
    "    subjects_names = np.load('/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_MCI.npy')\n",
    "    # filter the unified dictionary to retain just MCI\n",
    "    all_fMRI = {k:v['timeseries'] for k, v in all_dictionary.items() if k in subjects_names}\n",
    "    nsubjects = len(all_fMRI) \n",
    "\n",
    "    if mode == 'homogeneous':\n",
    "        wmBurden_dict = {k:v['total_WMH_load'] for k, v in all_dictionary.items() if k in subjects_names}\n",
    "        wmBurden = np.array([v for v in wmBurden_dict.values()])\n",
    "\n",
    "    elif mode == 'heterogeneous':\n",
    "        wmBurden_dict ={k:v['regional_WMH_load'] for k, v in all_dictionary.items() if k in subjects_names} \n",
    "        wmBurden = np.array([v for v in wmBurden_dict.values()])\n",
    "    else:\n",
    "        filter_MCI = np.load('/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/subject_list_wmh_volumes_MCI_minimal.npy', allow_pickle = True).flatten()\n",
    "        all_fMRI = {k:v for k, v in all_fMRI.items() if k not in filter_MCI}\n",
    "        wmBurden = np.zeros([78])\n",
    "\n",
    "if random:\n",
    "    np.random.shuffle(wmBurden)\n",
    "\n",
    "\n",
    "# We haven't filtered in Nilearn, so we are going to filter now.\n",
    "distanceSettings = {'FC': (FC, True), 'phFCD': (phFCD, True)}\n",
    "\n",
    "simulateBOLD.TR = 3.  # Recording interval: 1 sample every 3 seconds\n",
    "simulateBOLD.dt = 0.1 * simulateBOLD.TR / 2.\n",
    "simulateBOLD.Tmax = Tmax  # This is the length, in seconds\n",
    "simulateBOLD.dtt = simulateBOLD.TR  # We are not using milliseconds\n",
    "simulateBOLD.t_min = 10 * simulateBOLD.TR\n",
    "# simulateBOLD.recomputeTmaxneuronal() <- do not update Tmaxneuronal this way!\n",
    "# simulateBOLD.warmUpFactor = 6.\n",
    "simulateBOLD.Tmaxneuronal = (Tmax-1) * simulateBOLD.TR + 30\n",
    "integrator.ds = simulateBOLD.TR  # record every TR millisecond\n",
    "\n",
    "timeseries_condition_4freq = np.array([v for k,v in all_fMRI.items()])\n",
    "# Hopf.beta = 0.01\n",
    "f_diff = filtPowSpectr.filtPowSpetraMultipleSubjects(timeseries_condition_4freq, TR=3.)  # should be baseline_group_ts .. or baseling_group[0].reshape((1,52,193))\n",
    "f_diff[np.where(f_diff == 0)] = np.mean(f_diff[np.where(f_diff != 0)])  # f_diff(find(f_diff==0))=mean(f_diff(find(f_diff~=0)))\n",
    "\n",
    "Hopf.omega = 2 * np.pi * f_diff\n",
    "\n",
    "print(\"ADHopf Setup done!\")\n",
    "\n",
    "import os\n",
    "# Change the file to where you want to save results\n",
    "outFilePath = f'/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/10iterations_baseline_homogeneous_model_on_MCI_withoutWMH_subjectwise' \n",
    "\n",
    "if not os.path.exists(outFilePath):\n",
    "    os.makedirs(outFilePath)\n",
    "\n",
    "\n",
    "def fittingPipeline_homogeneous(subj_fMRI,\n",
    "                    distanceSettings,  # This is a dictionary of {name: (distance module, apply filters bool)}\n",
    "                    wms, wmBurden, subjectName):\n",
    "    print(\"\\n\\n###################################################################\")\n",
    "    print(\"# Fitting with ParmSeep\")\n",
    "    print(\"###################################################################\\n\")\n",
    "    # Now, optimize all we (G) values: determine optimal G to work with\n",
    "    wmParms = [{'a': base_a_value}]\n",
    "    fitting = ParmSeep.distanceForAll_Parms(subj_fMRI,\n",
    "                                            wmWs, \n",
    "                                            wmParms,\n",
    "                                            NumSimSubjects=10,\n",
    "                                            distanceSettings=distanceSettings,\n",
    "                                            parmLabel=f'a_fixed_',\n",
    "                                            fileNameSuffix='_'+subjectName,\n",
    "                                            outFilePath=outFilePath)\n",
    "\n",
    "    optimal = {sd: distanceSettings[sd][0].findMinMax(fitting[sd]) for sd in distanceSettings}\n",
    "    return optimal, fitting\n",
    "\n",
    "    subjectName = ''\n",
    "warmUp = True\n",
    "warmUpFactor = 10.\n",
    "Hopf.setParms({'we': 2.9})\n",
    "\n",
    "# This is the set of weights I used for all the simulations:\n",
    "# Set of weights for homogeneous model: wmWs = np.round(np.arange(-0.09,0.05,0.001), 4)\n",
    "#wmWs = np.round(np.arange(-0.08,0.0501,0.001), 4)\n",
    "wmWs = np.zeros(1)\n",
    "\n",
    "\n",
    "def fittingPipeline_heterogeneous(all_fMRI, wmBurden, wmWs):\n",
    "\n",
    "    best_parameters_dict = {}\n",
    "    fitting_parameters_dict = {}\n",
    "\n",
    "    for subjectName, subj_ts in all_fMRI.items():\n",
    "\n",
    "        subj_fMRI = {subjectName:subj_ts}\n",
    "        #wmBurden_subj = wmBurden_dict[subjectName]\n",
    "        best_parameters, fitting_parameters = fittingPipeline_homogeneous(subj_fMRI=subj_fMRI, distanceSettings=distanceSettings, subjectName=subjectName, wms=wmWs, wmBurden = wmBurden)\n",
    "        best_parameters_dict[subjectName] = best_parameters\n",
    "        fitting_parameters_dict[subjectName] = fitting_parameters\n",
    "\n",
    "    return best_parameters_dict, fitting_parameters_dict\n",
    "\n",
    "\n",
    "    \n",
    "best_parms_dict, fitting_parms_dict = fittingPipeline_heterogeneous(all_fMRI=all_fMRI, wmBurden=wmBurden, wmWs=wmWs)\n",
    "\n",
    "if not random:\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(outFilePath + f\"/{mode}_model_best_parameters_dictionary_{conditionToStudy}.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(best_parms_dict, f)\n",
    "    # close file\n",
    "    f.close()\n",
    "    # open file for writing, \"w\" \n",
    "    g = open(outFilePath + f\"/{mode}_model_fitting_parameters_dictionary_{conditionToStudy}.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(fitting_parms_dict, g)\n",
    "    # close file\n",
    "    g.close()\n",
    "\n",
    "else:\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(outFilePath + f\"/random_{mode}_model_best_parameters_dictionary_{conditionToStudy}.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(best_parms_dict, f)\n",
    "    # close file\n",
    "    f.close()\n",
    "    # open file for writing, \"w\" \n",
    "    g = open(outFilePath + f\"/random_{mode}_model_fitting_parameters_dictionary_{conditionToStudy}.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(fitting_parms_dict, g)\n",
    "    # close file\n",
    "    g.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepNeuroSeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d27f1c606f5bcda6f20c491fe8e38eaca7c3ef926190a630a4967b500ac1b94a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
