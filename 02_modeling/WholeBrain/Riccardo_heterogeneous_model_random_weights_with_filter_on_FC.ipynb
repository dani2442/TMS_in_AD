{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the packages needed for analyses\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Set up Hopf as our model \n",
    "import WholeBrain.Models.supHopf as Hopf\n",
    "from WholeBrain.simulate_SimOnly import Tmaxneuronal\n",
    "Hopf.initialValue = 0.1\n",
    "neuronalModel = Hopf\n",
    "\n",
    "# Set up our integrator\n",
    "import WholeBrain.Integrator_EulerMaruyama as myIntegrator\n",
    "integrator = myIntegrator\n",
    "integrator.neuronalModel = neuronalModel\n",
    "integrator.verbose = False\n",
    "integrator.clamping = False\n",
    "\n",
    "# Set up the integration parameters\n",
    "dt = 5e-5\n",
    "# tmax is equal to the number of timepoints: 193\n",
    "tmax= 193\n",
    "ds = 1e-4\n",
    "Tmaxneuronal = int((tmax+dt))\n",
    "\n",
    "import WholeBrain.simulate_SimOnly as simulateBOLD\n",
    "simulateBOLD.warmUp = True\n",
    "simulateBOLD.integrator = integrator\n",
    "simulateBOLD.warmUpFactor = 606./2000.\n",
    "\n",
    "# Set up the code to obtain the variables we want to maximize similarity to empirical FC\n",
    "import WholeBrain.Observables.FC as FC\n",
    "#import WholeBrain.Observables.swFCD as swFCD\n",
    "import WholeBrain.Observables.phFCD as phFCD\n",
    "import WholeBrain.Optimizers.ParmSeep as ParmSeep\n",
    "ParmSeep.simulateBOLD = simulateBOLD\n",
    "ParmSeep.integrator = integrator\n",
    "ParmSeep.verbose = True\n",
    "\n",
    "# set BOLD filter settings\n",
    "import WholeBrain.Utils.filteredPowerSpectralDensity as filtPowSpectr\n",
    "import WholeBrain.BOLDFilters as BOLDfilters\n",
    "\n",
    "# These filters are applied in the filtPowSpectr function that we use to extract the intrinsic frequencies of each region.\n",
    "# They are also applied to process the FC and swFCD and phFCD, but you can set the corresponding parameter to False later on. 0.04-0.07 Hz common to extract intrinsic frequencies\n",
    "BOLDfilters.flp = 0.04\n",
    "BOLDfilters.fhi = 0.07\n",
    "BOLDfilters.TR = 3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of names of all regions in the AAL atlas. This is needed to get the right indices, to then filter the FC\n",
    "import csv\n",
    "# This is a sublist of labels of the cortical regions that were included in the paper by Demirtas et al. - AAL atlas (78 regions, excluding infratentorial and deep)\n",
    "with open ('/home/riccardo/ADNI_Hopf/Utils/aal_regions_included.csv', newline='') as f:\n",
    "    new_reader = csv.reader(f)\n",
    "    included_regions = list(new_reader)\n",
    "f.close()\n",
    "\n",
    "# Get the AAL atlas labels\n",
    "import nilearn.datasets as datasets\n",
    "aal = datasets.fetch_atlas_aal()\n",
    "labels = np.array(aal.labels)\n",
    "# create an array with the indices of each label (note that these are not the label number from the nifti image)\n",
    "indices = np.array([i for i in enumerate(labels)])\n",
    "SC_regions_index = np.isin(labels, included_regions)\n",
    "# filter the indices that we want based on the position so to have a final SC matrix only for the regions we considered.\n",
    "SC_78_regions_aal_atlas = indices[SC_regions_index]\n",
    "filter_SC = np.array([int(i) for i in SC_78_regions_aal_atlas[:,0]])\n",
    "\n",
    "# Set file path for SC matrix\n",
    "x_path = '/home/riccardo/ADNI_Hopf/Utils/'\n",
    "# Load structural connectivity matrix and use it as parameter in Hopf model\n",
    "xfile = 'SCmatrices88healthy.mat' \n",
    "M = sio.loadmat(x_path + xfile); \n",
    "mat = M['SCmatrices']\n",
    "# averaging the SC among subjects\n",
    "mat0 = np.mean(mat,axis = 0)\n",
    "# Filter the SC to have just the 78 regions we considered\n",
    "x_mat0 = mat0[filter_SC]\n",
    "new_mat0 = x_mat0.T[filter_SC]\n",
    "# Prevent full synchronization of the model\n",
    "SCnorm = new_mat0 * 0.2 / new_mat0.max() \n",
    "np.fill_diagonal(SCnorm,0)\n",
    "print('SCnorm.shape={}'.format(new_mat0.shape))    \n",
    "Hopf.setParms({'SC':SCnorm})\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Retrieve the data for all subjects \n",
    "# ------------------------------------------------\n",
    "timeseries = np.load('/home/riccardo/ADNI_Hopf/Results/timeseries_HC.npy')\n",
    "nsubjects, nNodes, Tmax = timeseries.shape\n",
    "all_fMRI = {s: d for s,d in enumerate(timeseries)} \n",
    "# Since we aleardy filtered the data in the previous step from Nilearn, we aren't going to filter them again. Otherwise, a possible alternative, could be to add another\n",
    "# BOLDfilters to actually re-set the filters after the f_diff was extracted and before the call to the simulation.\n",
    "distanceSettings = {'FC': (FC, True), 'phFCD': (phFCD, True)}\n",
    "\n",
    "simulateBOLD.TR = 3.  # Recording interval: 1 sample every 3 seconds\n",
    "simulateBOLD.dt = 0.1 * simulateBOLD.TR / 2.\n",
    "simulateBOLD.Tmax = Tmax  # This is the length, in seconds\n",
    "simulateBOLD.dtt = simulateBOLD.TR  # We are not using milliseconds\n",
    "simulateBOLD.t_min = 10 * simulateBOLD.TR\n",
    "# simulateBOLD.recomputeTmaxneuronal() <- do not update Tmaxneuronal this way!\n",
    "# simulateBOLD.warmUpFactor = 6.\n",
    "simulateBOLD.Tmaxneuronal = (Tmax-1) * simulateBOLD.TR + 30\n",
    "integrator.ds = simulateBOLD.TR  # record every TR millisecond\n",
    "\n",
    "# Hopf.beta = 0.01\n",
    "f_diff = filtPowSpectr.filtPowSpetraMultipleSubjects(timeseries, TR=3.)  # should be baseline_group_ts .. or baseling_group[0].reshape((1,52,193))\n",
    "f_diff[np.where(f_diff == 0)] = np.mean(f_diff[np.where(f_diff != 0)])  # f_diff(find(f_diff==0))=mean(f_diff(find(f_diff~=0)))\n",
    "\n",
    "Hopf.omega = 2 * np.pi * f_diff\n",
    "\n",
    "print(\"ADHopf Setup done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_a_value = -0.02\n",
    "warmUp = True\n",
    "warmUpFactor = 10.\n",
    "\n",
    "def computeSubjectSimulation():\n",
    "    # integrator.neuronalModel.SC = C\n",
    "    # integrator.initBookkeeping(N, Tmaxneuronal)\n",
    "    if warmUp:\n",
    "        currObsVars = integrator.warmUpAndSimulate(dt, Tmaxneuronal, TWarmUp=Tmaxneuronal/warmUpFactor)\n",
    "    else:\n",
    "        currObsVars = integrator.simulate(dt, Tmaxneuronal)\n",
    "    # currObsVars = integrator.returnBookkeeping()  # curr_xn, curr_rn\n",
    "    neuro_act = currObsVars[:,1,:]  # curr_rn\n",
    "    return neuro_act\n",
    "    \n",
    "\n",
    "def loadXBurden(condition):\n",
    "    # ------------------- load and stack the different wm burdens\n",
    "    wm_hc = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_HC.npy')\n",
    "    wm_mci = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_MCI.npy')\n",
    "    wm_overall = np.load('/home/riccardo/ADNI_Hopf/Results/wmh_volumes_ALL.npy')\n",
    "    # ------------------- load the specific subject wm\n",
    "    if condition == 'hc':\n",
    "        wmBurden = wm_hc\n",
    "    elif condition == 'mci':\n",
    "        wmBurden = wm_mci\n",
    "    elif condition == 'all':\n",
    "        wmBurden = wm_overall\n",
    "    # ------------------- normalize and return\n",
    "    # wmBurdenNorm = (wmBurden - np.min(wmBurden))/np.ptp(wmBurden)  # Normalize each individual in [0,1]\n",
    "    wmBurdenNorm = (wmBurden - np.min(wm_overall))/np.ptp(wm_overall)  # Normalize the whole group in [0,1]\n",
    "    return wmBurdenNorm\n",
    "\n",
    "# ------------ load wm burden\n",
    "conditionToStudy='mci' #lower case, can be hc or mci or all\n",
    "wmBurden = loadXBurden(conditionToStudy)\n",
    "mode = 'random'  # homogeneous/heterogeneous/random\n",
    "\n",
    "# Note that this homogeneous is intended as homogeneous inside the same patient, so all regions of one patient have the same wmBurden, but different patients have different wmBurdens.\n",
    "# Heterogenous, instead, means that different regions in the same patient have different wmBurdens\n",
    "if mode == 'homogeneous':\n",
    "    #avgwm = np.average(wmBurden)\n",
    "    wmBurden = np.array([np.ones([nNodes]) * wmBurden[i] for i in range(len(wmBurden))])\n",
    "elif mode == 'random':\n",
    "    np.random.shuffle(wmBurden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFilePath = '/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/heterogeneous_model_across_subject_random_weights'\n",
    "\n",
    "def fittingPipeline_homogeneous(subj_fMRI,\n",
    "                    distanceSettings,  # This is a dictionary of {name: (distance module, apply filters bool)}\n",
    "                    wms, wmBurden, subjectName):\n",
    "    print(\"\\n\\n###################################################################\")\n",
    "    print(\"# Fitting with ParmSeep\")\n",
    "    print(\"###################################################################\\n\")\n",
    "    # Now, optimize all we (G) values: determine optimal G to work with\n",
    "    wmParms = [{'a': base_a_value + (wmW * wmBurden)} for wmW in wmWs]\n",
    "    fitting = ParmSeep.distanceForAll_Parms(subj_fMRI,\n",
    "                                            wmWs, \n",
    "                                            wmParms,\n",
    "                                            NumSimSubjects=1,\n",
    "                                            distanceSettings=distanceSettings,\n",
    "                                            parmLabel='a_random_between_subjects_',\n",
    "                                            fileNameSuffix='_'+subjectName,\n",
    "                                            outFilePath=outFilePath)\n",
    "\n",
    "    optimal = {sd: distanceSettings[sd][0].findMinMax(fitting[sd]) for sd in distanceSettings}\n",
    "    return optimal, fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conditionToStudy == 'hc':\n",
    "    subj_list = np.load('/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_HC.npy')\n",
    "elif conditionToStudy == 'mci':\n",
    "    subj_list = np.load('/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_MCI.npy')\n",
    "\n",
    "subjectName = ''\n",
    "warmUp = True\n",
    "warmUpFactor = 10.\n",
    "Hopf.setParms({'we': 2.9})\n",
    "wmWs = np.round(np.arange(-0.1,0.1,0.005), 4)\n",
    "\n",
    "\n",
    "def fittingPipeline_heterogeneous(all_fMRI, wmBurden, wmWs):\n",
    "    best_parameters_dict = {}\n",
    "    fitting_parameters_dict = {}\n",
    "\n",
    "    for k, subjectName in enumerate(subj_list[:8]):\n",
    "        subj_fMRI = {k:all_fMRI[k]}\n",
    "        wmBurden_subj = wmBurden[k]\n",
    "        best_parameters, fitting_parameters = fittingPipeline_homogeneous(subj_fMRI=subj_fMRI, distanceSettings=distanceSettings, subjectName=subjectName, wms=wmWs, wmBurden = wmBurden_subj)\n",
    "        best_parameters_dict[subjectName] = best_parameters\n",
    "        fitting_parameters_dict[subjectName] = fitting_parameters\n",
    "\n",
    "    return best_parameters_dict, fitting_parameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the memory load the simulations cannot be run all in the same batch, at least not in Jupyter for VSCode where I am working at the moment on my personal laptop. Luckily the code can be run in batches, this means that we can subdivide the whole simulation into 3 separate batches and run it 3 times. Each time you have to close down the notebook and reopen it. After the simulation is run and the files are saved for all patients we can just re-run the functions altogether and this will just load the already computed simulations. Also this crashes VSCode ( [it is a known bug](https://github.com/microsoft/vscode/issues/155242)), but we are still able to run the code, just we won't see any output on the screen, but we can save a combined dictionary as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_parms_dict, fitting_parms_dict = fittingPipeline_heterogeneous(all_fMRI=all_fMRI, wmBurden=wmBurden, wmWs=wmWs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conditionToStudy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mif\u001b[39;00m conditionToStudy \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[39m# open file for writing, \"w\" \u001b[39;00m\n\u001b[1;32m      6\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/random_model_best_parameters_dictionary_HC.pkl\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# write json object to file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conditionToStudy' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if conditionToStudy == 'hc':\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(\"/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/random_model_best_parameters_dictionary_HC.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(best_parms_dict, f)\n",
    "    # close file\n",
    "    f.close()\n",
    "    # open file for writing, \"w\" \n",
    "    g = open(\"/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/random_model_fitting_parameters_dictionary_HC.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(fitting_parms_dict, g)\n",
    "    # close file\n",
    "    g.close()\n",
    "\n",
    "elif conditionToStudy == 'mci':\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(\"/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/random_model_best_parameters_dictionary_MCI.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(best_parms_dict, f)\n",
    "    # close file\n",
    "    f.close()\n",
    "    # open file for writing, \"w\" \n",
    "    g = open(\"/home/riccardo/ADNI_Hopf/Results/G_fitted_to_HC-minimalWMH/random_model_fitting_parameters_dictionary_MCI.pkl\",\"wb\")\n",
    "    # write json object to file\n",
    "    pickle.dump(fitting_parms_dict, g)\n",
    "    # close file\n",
    "    g.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('neurolib': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e43de8b878494df763bc7045d8b6860d297bafdd730193e66e4d65bb98bce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
