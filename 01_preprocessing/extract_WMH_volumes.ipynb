{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee71ac1d",
   "metadata": {},
   "source": [
    "This script extracts WMH lesion loads both for global brain and for brain parcellated with the Hammers Atlas. \n",
    "Remember that the Hammers Atlas is in MNI space, so before using this script you have to make sure that you ran `transform_HammersAtlas_to_subject_space.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import nibabel.imagestats as nibstats\n",
    "\n",
    "from nilearn import plotting as niplot\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn import image as nimg\n",
    "\n",
    "from bids import BIDSLayout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "e2cc9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDS_dir = '/home/riccardo/ADNI_Hopf/Data/ALL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "6a40f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the layout variable to the BIDS-folder where your WMH are located\n",
    "layout = BIDSLayout(BIDS_dir, validate = False, config = ['bids', 'derivatives'])\n",
    "subjs = layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "864d1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_np = '/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_HC.npy'\n",
    "HC_subjs = np.load(HC_np)\n",
    "\n",
    "MCI_np = '/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_MCI.npy'\n",
    "MCI_subjs = np.load(MCI_np)\n",
    "\n",
    "ALL_np = '/home/riccardo/ADNI_Hopf/Results/subject_list_timeseries_all.npy'\n",
    "ALL_subjs = np.load(ALL_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "7da17741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through subjects, since not all subjects have WMH masks, perform WMH volume extraction only on those who have the WMH segmentation maskcalled '_label-WMH'\n",
    "\n",
    "def calculate_WMH_load(subj):\n",
    "    \n",
    "    wmh_mask = layout.get(subject = subj, datatype = 'anat', label = 'WMH', return_type='file')\n",
    "    if wmh_mask != list():\n",
    "        wmh = nib.load(wmh_mask[0])\n",
    "        wmh_volume = round(nibstats.mask_volume(wmh),1)\n",
    "    else:\n",
    "        wmh_volume = 0\n",
    "    return wmh_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "1536ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subj in subjs:\n",
    "\n",
    "#     wmh_mask = layout.get(subject = subj, datatype = 'anat', label = 'WMH', return_type='file')\n",
    "#     if wmh_mask != list():\n",
    "#         wmh = nib.load(wmh_mask[0])\n",
    "#         wmh_volume = round(nibstats.mask_volume(wmh),1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "85cabdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/home/riccardo/ADNI_Hopf/Results/'\n",
    "\n",
    "overall_WMH_dict = {}\n",
    "\n",
    "for subj in subjs:\n",
    "    overall_WMH_dict[subj] = calculate_WMH_load(subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "25a54b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WMH_subjs_list = []\n",
    "WMH_burden_list = []\n",
    "\n",
    "for k, v in overall_WMH_dict.items():\n",
    "    WMH_subjs_list.append(k)\n",
    "    WMH_burden_list.append(v)\n",
    "\n",
    "    WMH_burden_arr = np.array(WMH_burden_list)\n",
    "\n",
    "normalized_WMH_burden_series = np.round((WMH_burden_arr - np.min(WMH_burden_arr))/np.ptp(WMH_burden_arr), 5)  # Normalize the whole group in [0,1]\n",
    "\n",
    "\n",
    "overall_WMH_dict_normalized = {k: normalized_WMH_burden_series[n] for n, k in enumerate(WMH_subjs_list)}\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"/home/riccardo/ADNI_Hopf/Results/overall_WMH_burden_all_normalized.pkl\",\"wb\")\n",
    "# write json object to file\n",
    "pickle.dump(overall_WMH_dict_normalized, f)\n",
    "# close file\n",
    "f.close()\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "g = open(\"/home/riccardo/ADNI_Hopf/Results/overall_WMH_burden_all.pkl\",\"wb\")\n",
    "# write json object to file\n",
    "pickle.dump(overall_WMH_dict, g)\n",
    "# close file\n",
    "g.close()\n",
    "\n",
    "\n",
    "\n",
    "# WMH_condition_list = []\n",
    "\n",
    "# for k, v in overall_WMH_dict.items():\n",
    "#     WMH_subjs_list.append(k[1])\n",
    "#     WMH_burden_list.append(v)\n",
    "#     if k[1] in HC_subjs:\n",
    "#         WMH_condition_list.append('hc')\n",
    "#     elif k[1] in MCI_subjs:\n",
    "#         WMH_condition_list.append('mci')\n",
    "#     else:\n",
    "#         WMH_condition_list.append('excluded')\n",
    "\n",
    "# WMH_subjs_series = pd.Series(WMH_subjs_list, dtype = 'string')\n",
    "# WMH_burden_series = pd.Series(WMH_burden_list, dtype = 'float')\n",
    "# normalized_WMH_burden_series = (WMH_burden_series - np.min(WMH_burden_series))/np.ptp(WMH_burden_series)  # Normalize the whole group in [0,1]\n",
    "# WMH_condition_list = pd.Series(WMH_condition_list, dtype = 'string')\n",
    "\n",
    "# WMH_df = pd.DataFrame([WMH_subjs_series, WMH_burden_series, normalized_WMH_burden_series, WMH_condition_list]).T\n",
    "# WMH_df.columns = ['Subject', 'Total_WMH_load', 'Normalized_total_WMH_load', 'Condition']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "72422689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMH_df.to_csv('/home/riccardo/ADNI_Hopf/Results/WMH_overall_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3699f2",
   "metadata": {},
   "source": [
    "# WMH lesion load per parcellation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ccbce",
   "metadata": {},
   "source": [
    "The problem is that AAL and Desikan are cortical atlases, which makes sense since what we want are interactions between cortical regions, but our WMH loads are mainly subcortical.. So basically if I use AAL parcellations and I try to intersect them with my WMH lesion load I don't find anything because they do not intersect. Trying with the Hammers atlas that has WM as well as GM, could be a start. Downloaded from [here](http://brain-development.org/brain-atlases/adult-brain-atlases/adult-brain-maximum-probability-map-hammers-mith-atlas-n30r83-in-mni-space/#Download_-_Adult_brain_maximum_probability_mapWhite_matter_atlas_in_MNI-space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "685ca85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDS_dir = '/home/riccardo/WMH/ALL/'\n",
    "out_dir = '/home/riccardo/Documents/Hopf_model_AD/Results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "f6722519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels file from where you stored it\n",
    "hammer_labels_file = '/home/riccardo/ADNI_Hopf/Utils/Hammers67n20/n30r83_names_spreadsheet.csv'\n",
    "#get the labels numbers\n",
    "hammer_labels_df = pd.read_csv(hammer_labels_file, index_col=None)\n",
    "hammer_labels = hammer_labels_df.iloc[1:,0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "378d9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T1(subj):\n",
    "    '''Get the T1 file for the subject'''\n",
    "    all_t1_filenames = layout.get(subject = subj, datatype = 'anat', desc = 'preproc', extension = 'nii.gz', return_type='file')\n",
    "    mni2009_filenames = layout.get(subject = subj, datatype = 'anat', desc = 'preproc', space = 'MNI152NLin2009cAsym', extension = 'nii.gz', return_type='file')\n",
    "    mni6_filenames = layout.get(subject = subj, datatype = 'anat', desc = 'preproc', space = 'MNI152NLin6Asym', extension = 'nii.gz', return_type='file')\n",
    "    t1_filenames = [f for f in all_t1_filenames if f not in mni2009_filenames + mni6_filenames if 'FLAIR' not in f]\n",
    "    t1_filename = t1_filenames[0]\n",
    "    return t1_filename\n",
    "\n",
    "#define our function to extract WMH lesion load for each patient for all patients\n",
    "# def extract_regional_WMH_load(subj, t1_img, labels = hammer_labels):\n",
    "\n",
    "#     '''This function loops through the subjects in our BIDS folder and checks if there are WMH lesion masks available. If there are, it calculates the WMH lesion load for each\n",
    "#     parcellation in the HammersAtlas in subject space and returns an array containing the volume per region'''\n",
    "\n",
    "#     # Get the transformed Hammers atlas in subject space\n",
    "#     hammers_atlas_filenames = layout.get(subject = subj, datatype = 'anat', desc = 'brain', suffix = 'HammersAtlasParcellation', return_type='file')\n",
    "#     hammers_atlas_file_MNI = layout.get(subject = subj, datatype = 'anat', space = 'MNI152NLin2009cAsym', desc = 'brain', suffix = 'HammersAtlasParcellation', return_type='file')\n",
    "#     hammers_atlas_file = [hammers for hammers in hammers_atlas_filenames if hammers not in hammers_atlas_file_MNI]\n",
    "#     hammers_atlas = hammers_atlas_file[0]\n",
    "#     # Load the image\n",
    "#     hammers_img = nib.load(hammers_atlas)\n",
    "#     # Create an empty 1 X 5 numpy array to store the results of the WMH load for each of the considered regions (frontal, temporal, occipital, parietal, cingulum)\n",
    "#     subj_WMH = np.zeros([1, 5])\n",
    "#     # check if they have WMH, if not skip\n",
    "#     wmh_mask = layout.get(subject = subj, datatype = 'anat', label = 'WMH', return_type='file')\n",
    "#     if wmh_mask != list():\n",
    "#         wmh = nib.load(wmh_mask[0])\n",
    "\n",
    "#     # resample the atlas for this patient.... not sure if needed, need to tryout\n",
    "#         resamp_atlas = nimg.resample_to_img(hammers_img, t1_img, 'nearest')    \n",
    "#     # calculate WMH lesion load for each region of interest/parcellation for this patient\n",
    "\n",
    "#         for n, label in enumerate(labels):\n",
    "#             roi_array_bool = (resamp_atlas.get_fdata() == label)\n",
    "#             roi_current_array = roi_array_bool.astype(int)\n",
    "#             roi_current_mask = nib.Nifti1Image(roi_current_array, affine=hammers_img.affine)\n",
    "#             intersection_mask = nimg.math_img('a * b', a = roi_current_mask, b = wmh)\n",
    "#             subj_WMH[n] = np.array([round(nibstats.mask_volume(intersection_mask),1)])\n",
    "#     else:\n",
    "#         for m, label in enumerate(labels):\n",
    "#             subj_WMH[m] = 0\n",
    "#     #     #niplot.plot_roi(roi_img=intersection_mask, alpha=0.4, title=1)\n",
    "#     return subj_WMH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "3129e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs = layout.get_subjects()\n",
    "\n",
    "hammers_temporal_lobe_idx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 30, 31, 82, 83]\n",
    "hammers_frontal_lobe_idx = [28, 29, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 68, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 81 ]\n",
    "hammers_occipital_lobe_idx = [64, 65, 66, 67, 22, 23]\n",
    "hammers_parietal_lobe_idx = [60, 61, 62, 63, 32, 33]\n",
    "hammers_insula_cingulate_idx = [20, 21, 24, 25, 26, 27]\n",
    "\n",
    "def extract_regional_WMH_load(subj, t1_img, labels = hammer_labels):\n",
    "\n",
    "    hammers_atlas_filenames = layout.get(subject = subj, datatype = 'anat', desc = 'brain', suffix = 'HammersAtlasParcellation', return_type='file')\n",
    "    hammers_atlas_file_MNI = layout.get(subject = subj, datatype = 'anat', space = 'MNI152NLin2009cAsym', desc = 'brain', suffix = 'HammersAtlasParcellation', return_type='file')\n",
    "    hammers_atlas_file = [hammers for hammers in hammers_atlas_filenames if hammers not in hammers_atlas_file_MNI]\n",
    "    hammers_atlas = hammers_atlas_file[0]\n",
    "    # Load the image\n",
    "    hammers_img = nib.load(hammers_atlas)\n",
    "\n",
    "    # Create an empty 1 X 5 numpy array to store the results of the WMH load for each of the considered regions (frontal, temporal, occipital, parietal, cingulum)\n",
    "    subj_WMH = np.zeros([len(hammer_labels)])\n",
    "    # check if they have WMH, if not skip\n",
    "    wmh_mask = layout.get(subject = subj, datatype = 'anat', label = 'WMH', return_type='file')\n",
    "    if wmh_mask != list():\n",
    "        wmh = nib.load(wmh_mask[0])\n",
    "\n",
    "    # calculate WMH lesion load for each region of interest/parcellation for this patient\n",
    "\n",
    "        for n, label in enumerate(hammer_labels):\n",
    "            roi_array_bool = (hammers_img.get_fdata() == label)\n",
    "            roi_current_array = roi_array_bool.astype(int)\n",
    "            roi_current_mask = nib.Nifti1Image(roi_current_array, affine=hammers_img.affine)\n",
    "            intersection_mask = nimg.math_img('a * b', a = roi_current_mask, b = wmh)\n",
    "            subj_WMH[n] = np.array([round(nibstats.mask_volume(intersection_mask),1)])\n",
    "    else:\n",
    "        for m, label in enumerate(hammer_labels):\n",
    "            subj_WMH[m] = 0\n",
    "\n",
    "\n",
    "    WMH_temporal = []\n",
    "    WMH_frontal = []\n",
    "    WMH_parietal = []\n",
    "    WMH_occipital = []\n",
    "    WMH_insula_cingulate = []\n",
    "\n",
    "    for i, WMH in enumerate(subj_WMH):\n",
    "    \n",
    "        if i in hammers_temporal_lobe_idx:\n",
    "            WMH_temporal.append(WMH)\n",
    "        elif i in hammers_frontal_lobe_idx:\n",
    "            WMH_frontal.append(WMH)\n",
    "        elif i in hammers_parietal_lobe_idx:\n",
    "            WMH_parietal.append(WMH)\n",
    "        elif i in hammers_occipital_lobe_idx:\n",
    "            WMH_occipital.append(WMH)\n",
    "        elif i in hammers_insula_cingulate_idx:\n",
    "            WMH_insula_cingulate.append(WMH)\n",
    "\n",
    "    WMH_temporal_array = np.array(WMH_temporal)\n",
    "    WMH_frontal_array = np.array(WMH_frontal)\n",
    "    WMH_parietal_array = np.array(WMH_parietal)\n",
    "    WMH_occipital_array = np.array(WMH_occipital)\n",
    "    WMH_insula_cingulate_array = np.array(WMH_insula_cingulate)\n",
    "\n",
    "    lobe_WMH = {'temporal': np.round(WMH_temporal_array.mean(), 3), \n",
    "                'frontal': np.round(WMH_frontal_array.mean(), 3), \n",
    "                'parietal': np.round(WMH_parietal_array.mean(), 3),\n",
    "                'occipital': np.round(WMH_occipital_array.mean(), 3),\n",
    "                'insula-cingulate': np.round(WMH_insula_cingulate_array.mean(), 3)}\n",
    "                \n",
    "    return lobe_WMH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "55268bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject: sub-ADNI011S6367 (1/33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14522/1631462985.py:30: FutureWarning: Image data has type int64, which may cause incompatibilities with other tools. This will error in NiBabel 5.0. This warning can be silenced by passing the dtype argument to Nifti1Image().\n",
      "  roi_current_mask = nib.Nifti1Image(roi_current_array, affine=hammers_img.affine)\n",
      "/home/riccardo/anaconda3/envs/neurolib/lib/python3.9/site-packages/nilearn/image/image.py:1041: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.\n",
      "  return new_img_like(niimg, result, niimg.affine)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject: sub-ADNI037S6083 (2/33)\n",
      "Processing subject: sub-ADNI068S2187 (3/33)\n",
      "Processing subject: sub-ADNI002S4799 (4/33)\n",
      "Processing subject: sub-ADNI002S4654 (5/33)\n",
      "Processing subject: sub-ADNI002S5178 (6/33)\n",
      "Processing subject: sub-ADNI002S1155 (7/33)\n",
      "Processing subject: sub-ADNI037S4706 (8/33)\n",
      "Processing subject: sub-ADNI003S6014 (9/33)\n",
      "Processing subject: sub-ADNI003S6432 (10/33)\n",
      "Processing subject: sub-ADNI003S6606 (11/33)\n",
      "Processing subject: sub-ADNI037S4214 (12/33)\n",
      "Processing subject: sub-ADNI011S6618 (13/33)\n",
      "Processing subject: sub-ADNI003S6258 (14/33)\n",
      "Processing subject: sub-ADNI024S6385 (15/33)\n",
      "Processing subject: sub-ADNI024S4674 (16/33)\n",
      "Processing subject: sub-ADNI006S6651 (17/33)\n",
      "Processing subject: sub-ADNI100S4556 (18/33)\n",
      "Processing subject: sub-ADNI003S6259 (19/33)\n",
      "Processing subject: sub-ADNI019S6186 (20/33)\n",
      "Processing subject: sub-ADNI024S6033 (21/33)\n",
      "Processing subject: sub-ADNI003S6307 (22/33)\n",
      "Processing subject: sub-ADNI003S6268 (23/33)\n",
      "Processing subject: sub-ADNI003S4644 (24/33)\n",
      "Processing subject: sub-ADNI011S6465 (25/33)\n",
      "Processing subject: sub-ADNI018S2155 (26/33)\n",
      "Processing subject: sub-ADNI002S6456 (27/33)\n",
      "Processing subject: sub-ADNI018S6414 (28/33)\n",
      "Processing subject: sub-ADNI014S6424 (29/33)\n",
      "Processing subject: sub-ADNI070S6236 (30/33)\n",
      "Processing subject: sub-ADNI068S4431 (31/33)\n",
      "Processing subject: sub-ADNI006S6291 (32/33)\n",
      "Processing subject: sub-ADNI002S4229 (33/33)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "subjs = layout.get_subjects()\n",
    "\n",
    "subjects_WMH = {}\n",
    "\n",
    "for i, subj in enumerate(subjs):\n",
    "    print(f'Processing subject: sub-{subj} ({i+1}/{len(subjs)})')\n",
    "    t1_img = get_T1(subj)\n",
    "    subjects_WMH[subj] = extract_regional_WMH_load(subj, t1_img)  \n",
    "print('Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "688430c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# open the csv containing the names of the 78 cortical regions that we will use \n",
    "with open('/home/riccardo/ADNI_Hopf/Utils/aal_regions_included.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    aal_included_regions = list(reader)\n",
    "f.close()\n",
    "#create a 1-D array with the names so that it is more easily iterable\n",
    "aal_included_regions_array = np.array([aal_included_regions]).flatten()\n",
    "\n",
    "# Get the AAL atlas labels\n",
    "import nilearn.datasets as datasets\n",
    "aal = datasets.fetch_atlas_aal()\n",
    "labels = np.array(aal.labels)\n",
    "# create an array with the indices of each label (note that these are not the label number from the nifti image)\n",
    "indices = np.array([i for i in enumerate(labels)])\n",
    "SC_regions_index = np.isin(labels, aal_included_regions)\n",
    "# filter the indices that we want based on the position so to have a final SC matrix only for the regions we considered.\n",
    "SC_78_regions_aal_atlas = indices[SC_regions_index]\n",
    "filter_SC = np.array([int(i) for i in SC_78_regions_aal_atlas[:,0]])\n",
    "\n",
    "aal_frontal_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "aal_insula_cingulate_indices =[28, 29, 30, 31, 32, 33, 34, 35]\n",
    "aal_temporal_indices = [36, 37, 54, 55, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
    "aal_occipital_indices = [42, 43, 44, 45, 46, 47, 48, 49, 50 ,51, 52, 53]\n",
    "aal_parietal_indices = [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "793b1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aal_regional_indices = [aal_frontal_indices, aal_insula_cingulate_indices, aal_temporal_indices, aal_occipital_indices, aal_parietal_indices]\n",
    "aal_region_name = ['frontal', 'insula-cingulate', 'temporal', 'occipital', 'parietal']\n",
    "\n",
    "def get_lobe_WMH(aal_regional_indices, aal_region_name):\n",
    "\n",
    "    \"\"\"This function is used to convert the regional weights from each region of the Hammers atlas (that also has deep structures) into a 1 X 5 array containing the same weight\n",
    "    for each region that forms a lobe (e.g., all regions composing the frontal lobe get the same WMH weight)\n",
    "    \n",
    "    Inputs:\n",
    "    aal_regional_indices (list): a list of lists containing the indices that make up each lobe (e.g., [1,2,3,4,...] for the frontal lobe)\n",
    "    aal_region_name (list): a list of strings with the corresponding lobe names (e.g., ['frontal', 'parietal', etc.])\n",
    "\n",
    "    Returns:\n",
    "    WMH_all_arr (nd.array): a 1 X 78 array. For each element of the AAL atlas belonging to a lobe, the output is the average WMH load for that brain lobe.\n",
    "     \"\"\"\n",
    "\n",
    "    WMH_all = []\n",
    "\n",
    "    for n, v in enumerate(aal_regional_indices):\n",
    "        WMH_region_aal = np.ones(len(v))\n",
    "        for i, WMH in np.ndenumerate(WMH_region_aal):\n",
    "            WMH_region_aal[i] = WMH_region_aal[i] * subjects_WMH[subj][aal_region_name[n]]\n",
    "            WMH_all.append(WMH_region_aal[i])\n",
    "    WMH_all_arr = np.array(WMH_all)\n",
    "\n",
    "    return WMH_all_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "3695d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_WMH_lobewise_dict = {}\n",
    "\n",
    "for subj in subjs:\n",
    "    subjects_WMH_lobewise_dict[subj] = get_lobe_WMH(aal_regional_indices, aal_region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "20d7fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmBurden = np.array([])\n",
    "for k, v in subjects_WMH_lobewise_dict.items():\n",
    "    wmBurden = np.concatenate([wmBurden, v]) \n",
    "\n",
    "wmBurdenNorm = (wmBurden - np.min(wmBurden))/np.ptp(wmBurden)  # Normalize the whole group in [0,1]\n",
    "wmBurdenNorm_reshaped = wmBurdenNorm.reshape([len(subjects_WMH_lobewise_dict),78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d6372ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lobe_WMH_normalized_dict = {}\n",
    "\n",
    "for k, a in enumerate(subjects_WMH_lobewise_dict.items()):\n",
    "    lobe_WMH_normalized_dict[a[0]] = np.round(wmBurdenNorm_reshaped[k], 5)\n",
    "\n",
    "WMH_lobe_subjs = pd.Series(lobe_WMH_normalized_dict.keys(), dtype = 'string')\n",
    "lobe_WMH_load = pd.Series(lobe_WMH_normalized_dict.values(), dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "eeedc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to convert everything to a dataframe, but when I load it I lose the numpy array inside the df even though I pickle-save them.\n",
    "# WMH_lobe_condition_list = []\n",
    "\n",
    "# for k, v in lobe_WMH_normalized_dict.items():\n",
    "\n",
    "#     if k in HC_subjs:\n",
    "#         WMH_lobe_condition_list.append('hc')\n",
    "#     elif k in MCI_subjs:\n",
    "#         WMH_lobe_condition_list.append('mci')\n",
    "#     else:\n",
    "#         WMH_lobe_condition_list.append('excluded')\n",
    "\n",
    "# WMH_lobe_condition_series = pd.Series(WMH_lobe_condition_list, dtype = 'string')\n",
    "# WMH_lobe_weights_df = pd.DataFrame([WMH_lobe_subjs, lobe_WMH_load, WMH_lobe_condition_series]).T\n",
    "# WMH_lobe_weights_df.columns = ['Subject', 'wmBurden_array', 'Condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "5e234b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMH_lobe_weights_df.to_pickle('/home/riccardo/ADNI_Hopf/Results/WMH_heterogeneous_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "3ce9cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file for writing, \"w\" \n",
    "f = open(\"/home/riccardo/ADNI_Hopf/Results/normalized_WMH_lobewise_all.pkl\",\"wb\")\n",
    "\n",
    "# write json object to file\n",
    "pickle.dump(lobe_WMH_normalized_dict, f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35f7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for subj in subjs[:1]:\n",
    "\n",
    "#     WMH_frontal_aal = np.ones(len(aal_frontal_indices))\n",
    "#     for n, WMH in np.ndenumerate(WMH_frontal_aal):\n",
    "#         WMH_frontal_aal[n] = WMH_frontal_aal[n] * subjects_WMH[subj]['frontal']\n",
    "\n",
    "#     WMH_parietal_aal = np.ones(len(aal_parietal_indices))\n",
    "#     for n, WMH in np.ndenumerate(WMH_parietal_aal):\n",
    "#         WMH_parietal_aal[n] = WMH_parietal_aal[n] * subjects_WMH[subj]['parietal']\n",
    "\n",
    "#     WMH_temporal_aal = np.ones(len(aal_temporal_indices))\n",
    "#     for n, WMH in np.ndenumerate(WMH_temporal_aal):\n",
    "#         WMH_temporal_aal[n] = WMH_temporal_aal[n] * subjects_WMH[subj]['temporal']\n",
    "\n",
    "#     WMH_occipital_aal = np.ones(len(aal_occipital_indices))\n",
    "#     for n, WMH in np.ndenumerate(WMH_occipital_aal):\n",
    "#         WMH_occipital_aal[n] = WMH_occipital_aal[n] * subjects_WMH[subj]['occipital']\n",
    "\n",
    "#     WMH_insula_cingulate_aal = np.ones(len(aal_insula_cingulate_indices))\n",
    "#     for n, WMH in np.ndenumerate(WMH_insula_cingulate_aal):\n",
    "#         WMH_insula_cingulate_aal[n] = WMH_insula_cingulate_aal[n] * subjects_WMH[subj]['insula-cingulate']\n",
    "\n",
    "# allWMH = np.hstack([WMH_frontal_aal, WMH_insula_cingulate_aal, WMH_temporal_aal, WMH_occipital_aal, WMH_parietal_aal])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('neurolib': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "18e43de8b878494df763bc7045d8b6860d297bafdd730193e66e4d65bb98bce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
